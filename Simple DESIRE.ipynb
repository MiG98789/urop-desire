{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UROP\n",
    "\n",
    "## DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents\n",
    "\n",
    "[CVPR Paper](https://arxiv.org/pdf/1704.04394.pdf)\n",
    "\n",
    "[Supplementary Notes](http://www.robots.ox.ac.uk/~namhoon/doc/DESIRE-supp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "1) Synchronised batch\n",
    "\n",
    "2) Put everything into one big model\n",
    "\n",
    "3) Save and load model\n",
    "\n",
    "4) Train/test flag\n",
    "  * Train has output Y_hat and z\n",
    "  * Test has input z array and output Y_hat\n",
    "  \n",
    "5) Visualise loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_line_sep():\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data extract:\n",
      "['1', '172', '93.939', '-55.7615', '0', '-3.0066', '5.27149', '4', '10']\n",
      "--------------------------------------\n",
      "Filtered data extract:\n",
      "[1, 172, 93.939, -55.7615]\n",
      "--------------------------------------\n",
      "Data dictionary extract:\n",
      "[1, 93.939, -55.7615]\n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "with open('raw_data/raw_record.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    raw_data = list(csvreader)\n",
    "print('Raw data extract:')\n",
    "print(raw_data[0])\n",
    "print_line_sep()\n",
    "\n",
    "filtered_data = [[int(row[0]), int(row[1]), float(row[2]), float(row[3])] for row in raw_data]\n",
    "print('Filtered data extract:')\n",
    "print(filtered_data[0])\n",
    "print_line_sep()\n",
    "\n",
    "dict_data = {}\n",
    "for row in filtered_data:\n",
    "    time = row[0]\n",
    "    car = row[1]\n",
    "    x = row[2]\n",
    "    y = row[3]\n",
    "    \n",
    "    if car in dict_data:\n",
    "        dict_data[car].append([time, x, y])\n",
    "    else:\n",
    "        dict_data[car] = [[time, x, y]]\n",
    "print('Data dictionary extract:')\n",
    "print(dict_data[172][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_random_batches(x_seq_len, y_seq_len, batch_size):\n",
    "    batches = []\n",
    "    dict_data_keys = list(dict_data.keys())\n",
    "    \n",
    "    while(len(batches) < batch_size): # Fill up batches with batch_size rows of x+y_seq_len columns\n",
    "        car = random.choice(dict_data_keys)\n",
    "        batch = []\n",
    "        is_first = True\n",
    "        \n",
    "        # Loop through each row of that car until x+y_seq_len columns are found\n",
    "        for row in dict_data[car]:\n",
    "            time = row[0]\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            \n",
    "            # If first item of the sequence, just append\n",
    "            if is_first:\n",
    "                batch.append([time, x, y])\n",
    "                is_first = False\n",
    "                \n",
    "            # If not, check if time diff is 1\n",
    "            else:\n",
    "                prev_time = batch[-1][0]\n",
    "                \n",
    "                # If time diff is not 1,\n",
    "                # 1) Clear batch item\n",
    "                # 2) Start from current location as first batch item\n",
    "                if time - prev_time != 1:\n",
    "                    batch = []\n",
    "                    batch.append([time, x, y])\n",
    "                    \n",
    "                # Otherwise, just append to batch item\n",
    "                else:\n",
    "                    batch.append([time, x, y])\n",
    "            \n",
    "            # If batch item columns are enough, break\n",
    "            if len(batch) == x_seq_len + y_seq_len:\n",
    "                batches.append(batch)\n",
    "                break\n",
    "                \n",
    "    # Just keep (x,y)  \n",
    "    X_position_array = [[[item[1], item[2]] for item in batch[:x_seq_len]] for batch in batches]\n",
    "    Y_position_array = [[[item[1], item[2]] for item in batch[x_seq_len:]] for batch in batches]\n",
    "    \n",
    "    X_position_np = np.asarray(X_position_array).transpose(0, 2, 1)\n",
    "    Y_position_np = np.asarray(Y_position_array).transpose(0, 2, 1)\n",
    "    \n",
    "    X_position_tensor = Variable(torch.from_numpy(X_position_np).type(torch.FloatTensor)).cuda()\n",
    "    Y_position_tensor = Variable(torch.from_numpy(Y_position_np).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "    # Convert position to displacement\n",
    "    X_displacement_array = []\n",
    "    Y_displacement_array = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        x_first = X_position_array[i][0][0]\n",
    "        y_first = X_position_array[i][0][1]\n",
    "\n",
    "        X_batch = X_position_array[i]\n",
    "        X_displacement_array.append([[item[0] - x_first, item[1] - y_first] for item in X_batch])\n",
    "\n",
    "        Y_batch = Y_position_array[i]\n",
    "        Y_displacement_array.append([[item[0] - x_first, item[1] - y_first] for item in Y_batch])\n",
    "\n",
    "    X_displacement_np = np.asarray(X_displacement_array).transpose(0, 2, 1)    \n",
    "    Y_displacement_np = np.asarray(Y_displacement_array).transpose(0, 2, 1)\n",
    "    \n",
    "    X_displacement_tensor = Variable(torch.from_numpy(X_displacement_np).type(torch.FloatTensor)).cuda()\n",
    "    Y_displacement_tensor = Variable(torch.from_numpy(Y_displacement_np).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "    return X_position_tensor, Y_position_tensor, X_displacement_tensor, Y_displacement_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X positions:\n",
      "torch.Size([8, 2, 20])\n",
      "tensor([[ 338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340],\n",
      "        [-122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890]], device='cuda:0')\n",
      "--------------------------------------\n",
      "Y positions:\n",
      "torch.Size([8, 2, 40])\n",
      "tensor([[ 338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340,  338.7340,  338.7340,\n",
      "          338.7340,  338.7340,  338.7340,  338.7340],\n",
      "        [-122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890, -122.2890, -122.2890,\n",
      "         -122.2890, -122.2890, -122.2890, -122.2890]], device='cuda:0')\n",
      "--------------------------------------\n",
      "X displacements:\n",
      "torch.Size([8, 2, 20])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0')\n",
      "--------------------------------------\n",
      "Y displacements:\n",
      "torch.Size([8, 2, 40])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X_position_test, Y_position_test, X_displacement_test, Y_displacement_test = get_random_batches(20, 40, 8)\n",
    "\n",
    "print('X positions:')\n",
    "print(X_position_test.size())\n",
    "print(X_position_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('Y positions:')\n",
    "print(Y_position_test.size())\n",
    "print(Y_position_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('X displacements:')\n",
    "print(X_displacement_test.size())\n",
    "print(X_displacement_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('Y displacements:')\n",
    "print(Y_displacement_test.size())\n",
    "print(Y_displacement_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SampleEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, seq_len, num_layers,\n",
    "                 conv_output_dim, conv_kernel_size,\n",
    "                 gru_hidden_dim):\n",
    "        super(SampleEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_output_dim = conv_output_dim\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        # C = X or Y\n",
    "        # C_i, (input_dim, seq_len) -> tC_i, (conv_output_dim, seq_len)\n",
    "        self.conv = nn.Conv1d(input_dim, conv_output_dim, conv_kernel_size)\n",
    "\n",
    "        # tC_i, (conv_output_dim, seq_len) -> H_C_i, (gru_hidden_dim)\n",
    "        self.gru = nn.GRU(conv_output_dim, gru_hidden_dim, num_layers)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initial hidden vector is hidden_dim-dimensional and padded with 0\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.gru_hidden_dim)).cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        conv_output = self.conv(x)\n",
    "        conv_output = F.relu(conv_output)\n",
    "        \n",
    "        # conv_output has dimensions (batch_size, dim, seq_len)\n",
    "        # GRU accepts input tensor with dimensions (seq_len, batch_size, dim)\n",
    "        # TODO: Pad\n",
    "        conv_output = conv_output.permute(2, 0, 1)\n",
    "        \n",
    "        output, hidden = self.gru(conv_output, hidden)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SampleCVAE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, mu_dim, sigma_dim):\n",
    "        super(SampleCVAE, self).__init__()\n",
    "        \n",
    "        self.sigma_dim = sigma_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc_mu = nn.Linear(output_dim, mu_dim)\n",
    "        self.fc_sigma = nn.Linear(output_dim, sigma_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        mu = self.fc_mu(output)\n",
    "        sigma = torch.div(torch.exp(self.fc_sigma(output)), 2)\n",
    "        epsilon = Variable(torch.normal(torch.zeros(batch_size, self.sigma_dim),\n",
    "                           torch.ones(batch_size, self.sigma_dim))).cuda()\n",
    "        # Reparam trick\n",
    "        z = mu + sigma*epsilon\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SampleFCS(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SampleFCS, self).__init__()\n",
    "        \n",
    "        self.fcs = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fcs(x)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SampleDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, num_layers, gru_hidden_dim, gru_output_dim, output_dim):\n",
    "        super(SampleDecoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, gru_output_dim, num_layers)\n",
    "        self.linear = nn.Linear(gru_output_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.gru_hidden_dim)).cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "X_seq_len = 20\n",
    "Y_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "# input_dim = 2        | input is a sequence of (x,y) coordinates (i.e. 2-dimensional)\n",
    "# seq_len = 20         | time sequence length of 20\n",
    "# conv_output_dim = 16 | 1D convolution with 16 output channels\n",
    "# conv_kernel_size = 3 | 1D convolution with kernel of width 3\n",
    "# gru_hidden_dim = 48  | 48-dimensional hidden vector\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 2, 20)\n",
    "encoder1 = SampleEncoder(input_dim=2, seq_len=X_seq_len, num_layers=1,\n",
    "                         conv_output_dim=16, conv_kernel_size=3,\n",
    "                         gru_hidden_dim=48).cuda()\n",
    "\n",
    "# input_dim = 2        | input is a sequence of (x,y) coordinates (i.e. 2-dimensional)\n",
    "# seq_len = 40         | time sequence length of 40\n",
    "# conv_output_dim = 16 | 1D convolution with 16 output channels\n",
    "# conv_kernel_size = 1 | 1D convolution with kernel of width 1\n",
    "# gru_hidden_dim = 48  | 48-dimensional hidden vector\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 2, 40)\n",
    "encoder2 = SampleEncoder(input_dim=2, seq_len=Y_seq_len, num_layers=1,\n",
    "                         conv_output_dim=16, conv_kernel_size=1,\n",
    "                         gru_hidden_dim=48).cuda()\n",
    "\n",
    "# input_dim = 96       | Concatenate encoder1's and encoder2's outputs (48-dim each) into 1 output (96-dim)\n",
    "# output_dim = 48      | Transform concatenated 96-dim vector into 48-dim vector\n",
    "# mu_dim = 48          | mu is 48-dim\n",
    "# sigma_dim = 48       | sigma is 48-dim\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 48)\n",
    "cvae = SampleCVAE(input_dim=96, output_dim=48, mu_dim=48, sigma_dim=48).cuda()\n",
    "\n",
    "# input_dim = 48       |\n",
    "# output_dim = 48      |\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 48)\n",
    "fcs = SampleFCS(input_dim=48, output_dim=48).cuda()\n",
    "\n",
    "# input_dim = 48       |\n",
    "# seq_len = 40         | time sequence length of 40\n",
    "# gru_hidden_dim = 48  |\n",
    "# gru_output_dim = 48  |\n",
    "# output_dim = 2       |\n",
    "#\n",
    "# Input: Tensor of size (40, batch_size, 48)\n",
    "decoder1 = SampleDecoder(input_dim=48, seq_len=40, num_layers=1,\n",
    "                        gru_hidden_dim=48, gru_output_dim=48,\n",
    "                        output_dim=2).cuda()\n",
    "\n",
    "# Loss\n",
    "kld = nn.KLDivLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = optim.Adam([\n",
    "#     {'params': encoder1.parameters()},\n",
    "#     {'params': encoder2.parameters()},\n",
    "#     {'params': cvae.parameters()},\n",
    "#     {'params': fcs.parameters()},\n",
    "#     {'params': decoder1.parameters()}\n",
    "# ], lr=learning_rate\n",
    "# )\n",
    "\n",
    "encoder1_optimizer = optim.Adam(encoder1.parameters(), lr=learning_rate)\n",
    "encoder2_optimizer = optim.Adam(encoder2.parameters(), lr=learning_rate)\n",
    "cvae_optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "fcs_optimizer = optim.Adam(fcs.parameters(), lr=learning_rate)\n",
    "decoder1_optimizer = optim.Adam(decoder1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) Total Loss: 11.394, KLD Loss: 1.435, MSE Loss: 9.958\n",
      "(Epoch 2) Total Loss: 20.080, KLD Loss: 1.625, MSE Loss: 18.456\n",
      "(Epoch 3) Total Loss: 30.731, KLD Loss: 1.729, MSE Loss: 29.002\n",
      "(Epoch 4) Total Loss: 24.669, KLD Loss: 1.751, MSE Loss: 22.918\n",
      "(Epoch 5) Total Loss: 20.341, KLD Loss: 1.578, MSE Loss: 18.763\n",
      "(Epoch 6) Total Loss: 70.963, KLD Loss: 1.601, MSE Loss: 69.362\n",
      "(Epoch 7) Total Loss: 42.458, KLD Loss: 1.541, MSE Loss: 40.917\n",
      "(Epoch 8) Total Loss: 15.113, KLD Loss: 1.439, MSE Loss: 13.675\n",
      "(Epoch 9) Total Loss: 11.877, KLD Loss: 1.855, MSE Loss: 10.022\n",
      "(Epoch 10) Total Loss: 17.075, KLD Loss: 1.581, MSE Loss: 15.494\n",
      "(Epoch 11) Total Loss: 24.998, KLD Loss: 1.456, MSE Loss: 23.542\n",
      "(Epoch 12) Total Loss: 17.905, KLD Loss: 1.478, MSE Loss: 16.427\n",
      "(Epoch 13) Total Loss: 22.676, KLD Loss: 1.680, MSE Loss: 20.997\n",
      "(Epoch 14) Total Loss: 44.887, KLD Loss: 1.680, MSE Loss: 43.207\n",
      "(Epoch 15) Total Loss: 23.855, KLD Loss: 1.443, MSE Loss: 22.411\n",
      "(Epoch 16) Total Loss: 27.043, KLD Loss: 1.269, MSE Loss: 25.774\n",
      "(Epoch 17) Total Loss: 42.190, KLD Loss: 1.637, MSE Loss: 40.553\n",
      "(Epoch 18) Total Loss: 17.227, KLD Loss: 1.617, MSE Loss: 15.611\n",
      "(Epoch 19) Total Loss: 9.979, KLD Loss: 1.358, MSE Loss: 8.622\n",
      "(Epoch 20) Total Loss: 12.761, KLD Loss: 1.599, MSE Loss: 11.163\n",
      "(Epoch 21) Total Loss: 18.808, KLD Loss: 1.435, MSE Loss: 17.373\n",
      "(Epoch 22) Total Loss: 9.167, KLD Loss: 1.549, MSE Loss: 7.618\n",
      "(Epoch 23) Total Loss: 13.340, KLD Loss: 1.546, MSE Loss: 11.793\n",
      "(Epoch 24) Total Loss: 36.907, KLD Loss: 1.584, MSE Loss: 35.322\n",
      "(Epoch 25) Total Loss: 24.932, KLD Loss: 1.624, MSE Loss: 23.308\n",
      "(Epoch 26) Total Loss: 36.319, KLD Loss: 1.484, MSE Loss: 34.834\n",
      "(Epoch 27) Total Loss: 26.748, KLD Loss: 1.556, MSE Loss: 25.192\n",
      "(Epoch 28) Total Loss: 14.009, KLD Loss: 1.545, MSE Loss: 12.463\n",
      "(Epoch 29) Total Loss: 24.362, KLD Loss: 1.488, MSE Loss: 22.875\n",
      "(Epoch 30) Total Loss: 48.637, KLD Loss: 1.603, MSE Loss: 47.035\n",
      "(Epoch 31) Total Loss: 16.520, KLD Loss: 1.478, MSE Loss: 15.042\n",
      "(Epoch 32) Total Loss: 22.479, KLD Loss: 1.409, MSE Loss: 21.070\n",
      "(Epoch 33) Total Loss: 20.332, KLD Loss: 1.755, MSE Loss: 18.577\n",
      "(Epoch 34) Total Loss: 46.663, KLD Loss: 1.746, MSE Loss: 44.917\n",
      "(Epoch 35) Total Loss: 14.344, KLD Loss: 1.665, MSE Loss: 12.679\n",
      "(Epoch 36) Total Loss: 17.533, KLD Loss: 1.690, MSE Loss: 15.842\n",
      "(Epoch 37) Total Loss: 12.558, KLD Loss: 1.602, MSE Loss: 10.956\n",
      "(Epoch 38) Total Loss: 22.923, KLD Loss: 1.639, MSE Loss: 21.284\n",
      "(Epoch 39) Total Loss: 13.991, KLD Loss: 1.493, MSE Loss: 12.498\n",
      "(Epoch 40) Total Loss: 25.245, KLD Loss: 1.696, MSE Loss: 23.548\n",
      "(Epoch 41) Total Loss: 27.828, KLD Loss: 1.533, MSE Loss: 26.295\n",
      "(Epoch 42) Total Loss: 21.979, KLD Loss: 1.523, MSE Loss: 20.456\n",
      "(Epoch 43) Total Loss: 10.836, KLD Loss: 1.540, MSE Loss: 9.296\n",
      "(Epoch 44) Total Loss: 21.886, KLD Loss: 1.568, MSE Loss: 20.318\n",
      "(Epoch 45) Total Loss: 21.559, KLD Loss: 1.375, MSE Loss: 20.184\n",
      "(Epoch 46) Total Loss: 19.070, KLD Loss: 1.365, MSE Loss: 17.705\n",
      "(Epoch 47) Total Loss: 23.148, KLD Loss: 1.636, MSE Loss: 21.513\n",
      "(Epoch 48) Total Loss: 18.925, KLD Loss: 1.721, MSE Loss: 17.204\n",
      "(Epoch 49) Total Loss: 13.698, KLD Loss: 1.422, MSE Loss: 12.277\n",
      "(Epoch 50) Total Loss: 15.007, KLD Loss: 1.523, MSE Loss: 13.483\n",
      "(Epoch 51) Total Loss: 12.419, KLD Loss: 1.553, MSE Loss: 10.866\n",
      "(Epoch 52) Total Loss: 17.302, KLD Loss: 1.538, MSE Loss: 15.764\n",
      "(Epoch 53) Total Loss: 10.193, KLD Loss: 1.369, MSE Loss: 8.824\n",
      "(Epoch 54) Total Loss: 17.111, KLD Loss: 1.386, MSE Loss: 15.724\n",
      "(Epoch 55) Total Loss: 23.328, KLD Loss: 1.617, MSE Loss: 21.711\n",
      "(Epoch 56) Total Loss: 9.044, KLD Loss: 1.552, MSE Loss: 7.492\n",
      "(Epoch 57) Total Loss: 25.002, KLD Loss: 1.538, MSE Loss: 23.464\n",
      "(Epoch 58) Total Loss: 8.502, KLD Loss: 1.553, MSE Loss: 6.949\n",
      "(Epoch 59) Total Loss: 24.916, KLD Loss: 1.763, MSE Loss: 23.153\n",
      "(Epoch 60) Total Loss: 10.340, KLD Loss: 1.460, MSE Loss: 8.880\n",
      "(Epoch 61) Total Loss: 21.597, KLD Loss: 1.394, MSE Loss: 20.203\n",
      "(Epoch 62) Total Loss: 10.808, KLD Loss: 1.725, MSE Loss: 9.083\n",
      "(Epoch 63) Total Loss: 20.722, KLD Loss: 1.629, MSE Loss: 19.093\n",
      "(Epoch 64) Total Loss: 18.775, KLD Loss: 1.657, MSE Loss: 17.117\n",
      "(Epoch 65) Total Loss: 11.242, KLD Loss: 1.532, MSE Loss: 9.710\n",
      "(Epoch 66) Total Loss: 19.462, KLD Loss: 1.497, MSE Loss: 17.965\n",
      "(Epoch 67) Total Loss: 30.238, KLD Loss: 1.463, MSE Loss: 28.775\n",
      "(Epoch 68) Total Loss: 11.015, KLD Loss: 1.455, MSE Loss: 9.560\n",
      "(Epoch 69) Total Loss: 8.360, KLD Loss: 1.684, MSE Loss: 6.676\n",
      "(Epoch 70) Total Loss: 30.446, KLD Loss: 1.645, MSE Loss: 28.801\n",
      "(Epoch 71) Total Loss: 19.837, KLD Loss: 1.454, MSE Loss: 18.383\n",
      "(Epoch 72) Total Loss: 31.112, KLD Loss: 1.820, MSE Loss: 29.292\n",
      "(Epoch 73) Total Loss: 31.746, KLD Loss: 1.726, MSE Loss: 30.020\n",
      "(Epoch 74) Total Loss: 6.684, KLD Loss: 1.641, MSE Loss: 5.043\n",
      "(Epoch 75) Total Loss: 40.453, KLD Loss: 1.559, MSE Loss: 38.894\n",
      "(Epoch 76) Total Loss: 24.312, KLD Loss: 1.764, MSE Loss: 22.548\n",
      "(Epoch 77) Total Loss: 32.066, KLD Loss: 1.480, MSE Loss: 30.586\n",
      "(Epoch 78) Total Loss: 17.506, KLD Loss: 1.477, MSE Loss: 16.029\n",
      "(Epoch 79) Total Loss: 24.603, KLD Loss: 1.787, MSE Loss: 22.817\n",
      "(Epoch 80) Total Loss: 24.567, KLD Loss: 1.568, MSE Loss: 22.999\n",
      "(Epoch 81) Total Loss: 18.739, KLD Loss: 1.506, MSE Loss: 17.234\n",
      "(Epoch 82) Total Loss: 48.296, KLD Loss: 1.570, MSE Loss: 46.725\n",
      "(Epoch 83) Total Loss: 18.312, KLD Loss: 1.625, MSE Loss: 16.687\n",
      "(Epoch 84) Total Loss: 16.800, KLD Loss: 1.291, MSE Loss: 15.509\n",
      "(Epoch 85) Total Loss: 13.106, KLD Loss: 1.962, MSE Loss: 11.143\n",
      "(Epoch 86) Total Loss: 19.353, KLD Loss: 1.620, MSE Loss: 17.734\n",
      "(Epoch 87) Total Loss: 15.619, KLD Loss: 1.732, MSE Loss: 13.887\n",
      "(Epoch 88) Total Loss: 5.441, KLD Loss: 1.511, MSE Loss: 3.930\n",
      "(Epoch 89) Total Loss: 10.876, KLD Loss: 1.672, MSE Loss: 9.205\n",
      "(Epoch 90) Total Loss: 18.360, KLD Loss: 1.565, MSE Loss: 16.794\n",
      "(Epoch 91) Total Loss: 30.997, KLD Loss: 1.695, MSE Loss: 29.302\n",
      "(Epoch 92) Total Loss: 14.007, KLD Loss: 1.592, MSE Loss: 12.414\n",
      "(Epoch 93) Total Loss: 15.249, KLD Loss: 1.511, MSE Loss: 13.738\n",
      "(Epoch 94) Total Loss: 12.093, KLD Loss: 1.412, MSE Loss: 10.681\n",
      "(Epoch 95) Total Loss: 20.682, KLD Loss: 1.544, MSE Loss: 19.138\n",
      "(Epoch 96) Total Loss: 9.092, KLD Loss: 1.483, MSE Loss: 7.609\n",
      "(Epoch 97) Total Loss: 17.001, KLD Loss: 1.715, MSE Loss: 15.285\n",
      "(Epoch 98) Total Loss: 7.979, KLD Loss: 1.525, MSE Loss: 6.454\n",
      "(Epoch 99) Total Loss: 16.677, KLD Loss: 1.549, MSE Loss: 15.128\n",
      "(Epoch 100) Total Loss: 21.498, KLD Loss: 1.805, MSE Loss: 19.692\n",
      "(Epoch 101) Total Loss: 13.563, KLD Loss: 1.447, MSE Loss: 12.116\n",
      "(Epoch 102) Total Loss: 40.729, KLD Loss: 1.650, MSE Loss: 39.080\n",
      "(Epoch 103) Total Loss: 10.640, KLD Loss: 1.690, MSE Loss: 8.949\n",
      "(Epoch 104) Total Loss: 10.081, KLD Loss: 1.525, MSE Loss: 8.556\n",
      "(Epoch 105) Total Loss: 12.743, KLD Loss: 1.316, MSE Loss: 11.427\n",
      "(Epoch 106) Total Loss: 13.458, KLD Loss: 1.735, MSE Loss: 11.722\n",
      "(Epoch 107) Total Loss: 19.973, KLD Loss: 1.586, MSE Loss: 18.386\n",
      "(Epoch 108) Total Loss: 22.789, KLD Loss: 1.750, MSE Loss: 21.038\n",
      "(Epoch 109) Total Loss: 16.542, KLD Loss: 1.553, MSE Loss: 14.989\n",
      "(Epoch 110) Total Loss: 13.236, KLD Loss: 1.776, MSE Loss: 11.460\n",
      "(Epoch 111) Total Loss: 8.773, KLD Loss: 1.512, MSE Loss: 7.261\n",
      "(Epoch 112) Total Loss: 17.544, KLD Loss: 1.367, MSE Loss: 16.177\n",
      "(Epoch 113) Total Loss: 16.969, KLD Loss: 1.554, MSE Loss: 15.416\n",
      "(Epoch 114) Total Loss: 7.360, KLD Loss: 1.604, MSE Loss: 5.756\n",
      "(Epoch 115) Total Loss: 16.476, KLD Loss: 1.514, MSE Loss: 14.961\n",
      "(Epoch 116) Total Loss: 38.507, KLD Loss: 1.607, MSE Loss: 36.900\n",
      "(Epoch 117) Total Loss: 11.874, KLD Loss: 1.531, MSE Loss: 10.343\n",
      "(Epoch 118) Total Loss: 31.853, KLD Loss: 1.637, MSE Loss: 30.216\n",
      "(Epoch 119) Total Loss: 20.293, KLD Loss: 1.589, MSE Loss: 18.705\n",
      "(Epoch 120) Total Loss: 8.648, KLD Loss: 1.626, MSE Loss: 7.022\n",
      "(Epoch 121) Total Loss: 15.663, KLD Loss: 1.640, MSE Loss: 14.024\n",
      "(Epoch 122) Total Loss: 17.615, KLD Loss: 1.645, MSE Loss: 15.971\n",
      "(Epoch 123) Total Loss: 16.427, KLD Loss: 1.713, MSE Loss: 14.714\n",
      "(Epoch 124) Total Loss: 12.870, KLD Loss: 1.476, MSE Loss: 11.394\n",
      "(Epoch 125) Total Loss: 12.727, KLD Loss: 1.984, MSE Loss: 10.742\n",
      "(Epoch 126) Total Loss: 11.502, KLD Loss: 1.589, MSE Loss: 9.914\n",
      "(Epoch 127) Total Loss: 15.792, KLD Loss: 1.462, MSE Loss: 14.330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 128) Total Loss: 30.328, KLD Loss: 1.382, MSE Loss: 28.946\n",
      "(Epoch 129) Total Loss: 16.147, KLD Loss: 1.610, MSE Loss: 14.536\n",
      "(Epoch 130) Total Loss: 17.082, KLD Loss: 1.665, MSE Loss: 15.417\n",
      "(Epoch 131) Total Loss: 21.350, KLD Loss: 1.896, MSE Loss: 19.454\n",
      "(Epoch 132) Total Loss: 13.987, KLD Loss: 1.611, MSE Loss: 12.376\n",
      "(Epoch 133) Total Loss: 14.064, KLD Loss: 1.488, MSE Loss: 12.576\n",
      "(Epoch 134) Total Loss: 10.516, KLD Loss: 1.662, MSE Loss: 8.855\n",
      "(Epoch 135) Total Loss: 12.691, KLD Loss: 1.549, MSE Loss: 11.142\n",
      "(Epoch 136) Total Loss: 7.202, KLD Loss: 1.678, MSE Loss: 5.523\n",
      "(Epoch 137) Total Loss: 15.622, KLD Loss: 1.703, MSE Loss: 13.919\n",
      "(Epoch 138) Total Loss: 24.629, KLD Loss: 1.669, MSE Loss: 22.960\n",
      "(Epoch 139) Total Loss: 16.619, KLD Loss: 1.384, MSE Loss: 15.235\n",
      "(Epoch 140) Total Loss: 22.549, KLD Loss: 1.640, MSE Loss: 20.909\n",
      "(Epoch 141) Total Loss: 14.130, KLD Loss: 1.425, MSE Loss: 12.704\n",
      "(Epoch 142) Total Loss: 12.428, KLD Loss: 1.604, MSE Loss: 10.824\n",
      "(Epoch 143) Total Loss: 29.189, KLD Loss: 1.361, MSE Loss: 27.828\n",
      "(Epoch 144) Total Loss: 11.908, KLD Loss: 1.508, MSE Loss: 10.399\n",
      "(Epoch 145) Total Loss: 32.050, KLD Loss: 1.611, MSE Loss: 30.439\n",
      "(Epoch 146) Total Loss: 16.012, KLD Loss: 1.638, MSE Loss: 14.374\n",
      "(Epoch 147) Total Loss: 11.361, KLD Loss: 1.489, MSE Loss: 9.872\n",
      "(Epoch 148) Total Loss: 9.794, KLD Loss: 1.727, MSE Loss: 8.067\n",
      "(Epoch 149) Total Loss: 23.843, KLD Loss: 1.648, MSE Loss: 22.195\n",
      "(Epoch 150) Total Loss: 7.024, KLD Loss: 1.661, MSE Loss: 5.363\n",
      "(Epoch 151) Total Loss: 5.482, KLD Loss: 1.481, MSE Loss: 4.000\n",
      "(Epoch 152) Total Loss: 11.568, KLD Loss: 1.515, MSE Loss: 10.054\n",
      "(Epoch 153) Total Loss: 6.395, KLD Loss: 1.820, MSE Loss: 4.575\n",
      "(Epoch 154) Total Loss: 10.013, KLD Loss: 1.537, MSE Loss: 8.476\n",
      "(Epoch 155) Total Loss: 13.319, KLD Loss: 1.598, MSE Loss: 11.721\n",
      "(Epoch 156) Total Loss: 11.291, KLD Loss: 1.901, MSE Loss: 9.390\n",
      "(Epoch 157) Total Loss: 8.894, KLD Loss: 1.703, MSE Loss: 7.191\n",
      "(Epoch 158) Total Loss: 12.104, KLD Loss: 1.503, MSE Loss: 10.600\n",
      "(Epoch 159) Total Loss: 29.708, KLD Loss: 1.831, MSE Loss: 27.877\n",
      "(Epoch 160) Total Loss: 3.818, KLD Loss: 1.717, MSE Loss: 2.101\n",
      "(Epoch 161) Total Loss: 6.349, KLD Loss: 1.597, MSE Loss: 4.752\n",
      "(Epoch 162) Total Loss: 7.896, KLD Loss: 1.668, MSE Loss: 6.228\n",
      "(Epoch 163) Total Loss: 6.334, KLD Loss: 1.602, MSE Loss: 4.731\n",
      "(Epoch 164) Total Loss: 6.123, KLD Loss: 1.380, MSE Loss: 4.743\n",
      "(Epoch 165) Total Loss: 24.134, KLD Loss: 1.596, MSE Loss: 22.538\n",
      "(Epoch 166) Total Loss: 8.074, KLD Loss: 1.269, MSE Loss: 6.805\n",
      "(Epoch 167) Total Loss: 16.362, KLD Loss: 1.586, MSE Loss: 14.777\n",
      "(Epoch 168) Total Loss: 9.956, KLD Loss: 1.565, MSE Loss: 8.391\n",
      "(Epoch 169) Total Loss: 11.574, KLD Loss: 1.585, MSE Loss: 9.989\n",
      "(Epoch 170) Total Loss: 15.242, KLD Loss: 1.674, MSE Loss: 13.568\n",
      "(Epoch 171) Total Loss: 13.864, KLD Loss: 1.391, MSE Loss: 12.473\n",
      "(Epoch 172) Total Loss: 8.527, KLD Loss: 1.564, MSE Loss: 6.963\n",
      "(Epoch 173) Total Loss: 15.621, KLD Loss: 1.648, MSE Loss: 13.973\n",
      "(Epoch 174) Total Loss: 17.704, KLD Loss: 1.671, MSE Loss: 16.034\n",
      "(Epoch 175) Total Loss: 11.593, KLD Loss: 1.650, MSE Loss: 9.942\n",
      "(Epoch 176) Total Loss: 4.135, KLD Loss: 1.447, MSE Loss: 2.688\n",
      "(Epoch 177) Total Loss: 15.668, KLD Loss: 1.511, MSE Loss: 14.157\n",
      "(Epoch 178) Total Loss: 13.768, KLD Loss: 1.694, MSE Loss: 12.075\n",
      "(Epoch 179) Total Loss: 9.755, KLD Loss: 1.681, MSE Loss: 8.074\n",
      "(Epoch 180) Total Loss: 6.836, KLD Loss: 1.703, MSE Loss: 5.133\n",
      "(Epoch 181) Total Loss: 13.011, KLD Loss: 1.662, MSE Loss: 11.348\n",
      "(Epoch 182) Total Loss: 9.337, KLD Loss: 1.526, MSE Loss: 7.811\n",
      "(Epoch 183) Total Loss: 7.245, KLD Loss: 1.572, MSE Loss: 5.673\n",
      "(Epoch 184) Total Loss: 9.774, KLD Loss: 1.693, MSE Loss: 8.081\n",
      "(Epoch 185) Total Loss: 11.081, KLD Loss: 1.477, MSE Loss: 9.604\n",
      "(Epoch 186) Total Loss: 6.941, KLD Loss: 1.602, MSE Loss: 5.338\n",
      "(Epoch 187) Total Loss: 21.660, KLD Loss: 1.432, MSE Loss: 20.228\n",
      "(Epoch 188) Total Loss: 9.481, KLD Loss: 1.402, MSE Loss: 8.079\n",
      "(Epoch 189) Total Loss: 11.924, KLD Loss: 1.685, MSE Loss: 10.239\n",
      "(Epoch 190) Total Loss: 11.880, KLD Loss: 1.515, MSE Loss: 10.365\n",
      "(Epoch 191) Total Loss: 11.793, KLD Loss: 1.753, MSE Loss: 10.040\n",
      "(Epoch 192) Total Loss: 9.984, KLD Loss: 1.503, MSE Loss: 8.481\n",
      "(Epoch 193) Total Loss: 8.463, KLD Loss: 1.924, MSE Loss: 6.539\n",
      "(Epoch 194) Total Loss: 8.762, KLD Loss: 1.490, MSE Loss: 7.273\n",
      "(Epoch 195) Total Loss: 1.686, KLD Loss: 1.551, MSE Loss: 0.135\n",
      "(Epoch 196) Total Loss: 11.952, KLD Loss: 1.768, MSE Loss: 10.185\n",
      "(Epoch 197) Total Loss: 8.711, KLD Loss: 1.623, MSE Loss: 7.088\n",
      "(Epoch 198) Total Loss: 4.485, KLD Loss: 1.270, MSE Loss: 3.216\n",
      "(Epoch 199) Total Loss: 26.502, KLD Loss: 1.439, MSE Loss: 25.062\n",
      "(Epoch 200) Total Loss: 27.591, KLD Loss: 1.551, MSE Loss: 26.040\n",
      "(Epoch 201) Total Loss: 5.494, KLD Loss: 1.631, MSE Loss: 3.862\n",
      "(Epoch 202) Total Loss: 7.126, KLD Loss: 1.577, MSE Loss: 5.549\n",
      "(Epoch 203) Total Loss: 10.180, KLD Loss: 1.663, MSE Loss: 8.517\n",
      "(Epoch 204) Total Loss: 6.431, KLD Loss: 1.689, MSE Loss: 4.741\n",
      "(Epoch 205) Total Loss: 7.866, KLD Loss: 1.398, MSE Loss: 6.468\n",
      "(Epoch 206) Total Loss: 14.648, KLD Loss: 1.721, MSE Loss: 12.928\n",
      "(Epoch 207) Total Loss: 5.050, KLD Loss: 1.563, MSE Loss: 3.487\n",
      "(Epoch 208) Total Loss: 11.661, KLD Loss: 1.720, MSE Loss: 9.940\n",
      "(Epoch 209) Total Loss: 7.955, KLD Loss: 1.627, MSE Loss: 6.328\n",
      "(Epoch 210) Total Loss: 7.210, KLD Loss: 1.602, MSE Loss: 5.608\n",
      "(Epoch 211) Total Loss: 13.809, KLD Loss: 1.585, MSE Loss: 12.225\n",
      "(Epoch 212) Total Loss: 7.768, KLD Loss: 1.693, MSE Loss: 6.075\n",
      "(Epoch 213) Total Loss: 24.214, KLD Loss: 1.270, MSE Loss: 22.943\n",
      "(Epoch 214) Total Loss: 12.255, KLD Loss: 1.650, MSE Loss: 10.605\n",
      "(Epoch 215) Total Loss: 8.270, KLD Loss: 1.746, MSE Loss: 6.523\n",
      "(Epoch 216) Total Loss: 26.702, KLD Loss: 1.672, MSE Loss: 25.030\n",
      "(Epoch 217) Total Loss: 18.828, KLD Loss: 1.607, MSE Loss: 17.221\n",
      "(Epoch 218) Total Loss: 11.617, KLD Loss: 1.591, MSE Loss: 10.026\n",
      "(Epoch 219) Total Loss: 15.330, KLD Loss: 1.515, MSE Loss: 13.815\n",
      "(Epoch 220) Total Loss: 6.080, KLD Loss: 1.451, MSE Loss: 4.629\n",
      "(Epoch 221) Total Loss: 12.914, KLD Loss: 1.718, MSE Loss: 11.197\n",
      "(Epoch 222) Total Loss: 7.207, KLD Loss: 1.665, MSE Loss: 5.542\n",
      "(Epoch 223) Total Loss: 10.890, KLD Loss: 1.792, MSE Loss: 9.098\n",
      "(Epoch 224) Total Loss: 3.764, KLD Loss: 1.615, MSE Loss: 2.149\n",
      "(Epoch 225) Total Loss: 9.289, KLD Loss: 1.796, MSE Loss: 7.493\n",
      "(Epoch 226) Total Loss: 11.681, KLD Loss: 1.536, MSE Loss: 10.146\n",
      "(Epoch 227) Total Loss: 32.488, KLD Loss: 1.880, MSE Loss: 30.608\n",
      "(Epoch 228) Total Loss: 4.862, KLD Loss: 1.860, MSE Loss: 3.002\n",
      "(Epoch 229) Total Loss: 8.476, KLD Loss: 1.533, MSE Loss: 6.943\n",
      "(Epoch 230) Total Loss: 12.730, KLD Loss: 1.379, MSE Loss: 11.352\n",
      "(Epoch 231) Total Loss: 8.327, KLD Loss: 1.815, MSE Loss: 6.512\n",
      "(Epoch 232) Total Loss: 12.904, KLD Loss: 1.468, MSE Loss: 11.435\n",
      "(Epoch 233) Total Loss: 10.275, KLD Loss: 1.840, MSE Loss: 8.435\n",
      "(Epoch 234) Total Loss: 7.746, KLD Loss: 1.505, MSE Loss: 6.240\n",
      "(Epoch 235) Total Loss: 14.580, KLD Loss: 1.563, MSE Loss: 13.017\n",
      "(Epoch 236) Total Loss: 7.249, KLD Loss: 1.635, MSE Loss: 5.614\n",
      "(Epoch 237) Total Loss: 9.302, KLD Loss: 1.492, MSE Loss: 7.810\n",
      "(Epoch 238) Total Loss: 15.075, KLD Loss: 1.970, MSE Loss: 13.105\n",
      "(Epoch 239) Total Loss: 27.522, KLD Loss: 1.774, MSE Loss: 25.748\n",
      "(Epoch 240) Total Loss: 23.786, KLD Loss: 1.574, MSE Loss: 22.212\n",
      "(Epoch 241) Total Loss: 11.248, KLD Loss: 1.527, MSE Loss: 9.721\n",
      "(Epoch 242) Total Loss: 9.759, KLD Loss: 1.754, MSE Loss: 8.005\n",
      "(Epoch 243) Total Loss: 8.882, KLD Loss: 1.538, MSE Loss: 7.345\n",
      "(Epoch 244) Total Loss: 7.168, KLD Loss: 1.625, MSE Loss: 5.543\n",
      "(Epoch 245) Total Loss: 7.958, KLD Loss: 1.647, MSE Loss: 6.312\n",
      "(Epoch 246) Total Loss: 13.250, KLD Loss: 1.772, MSE Loss: 11.478\n",
      "(Epoch 247) Total Loss: 11.559, KLD Loss: 1.633, MSE Loss: 9.926\n",
      "(Epoch 248) Total Loss: 8.675, KLD Loss: 1.768, MSE Loss: 6.907\n",
      "(Epoch 249) Total Loss: 5.712, KLD Loss: 1.741, MSE Loss: 3.971\n",
      "(Epoch 250) Total Loss: 6.792, KLD Loss: 1.754, MSE Loss: 5.038\n",
      "(Epoch 251) Total Loss: 6.989, KLD Loss: 1.612, MSE Loss: 5.377\n",
      "(Epoch 252) Total Loss: 9.184, KLD Loss: 1.841, MSE Loss: 7.343\n",
      "(Epoch 253) Total Loss: 8.706, KLD Loss: 1.600, MSE Loss: 7.106\n",
      "(Epoch 254) Total Loss: 7.796, KLD Loss: 1.674, MSE Loss: 6.122\n",
      "(Epoch 255) Total Loss: 5.634, KLD Loss: 1.695, MSE Loss: 3.939\n",
      "(Epoch 256) Total Loss: 21.938, KLD Loss: 1.612, MSE Loss: 20.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 257) Total Loss: 7.571, KLD Loss: 1.867, MSE Loss: 5.704\n",
      "(Epoch 258) Total Loss: 8.880, KLD Loss: 1.763, MSE Loss: 7.117\n",
      "(Epoch 259) Total Loss: 6.696, KLD Loss: 1.645, MSE Loss: 5.050\n",
      "(Epoch 260) Total Loss: 5.066, KLD Loss: 1.606, MSE Loss: 3.461\n",
      "(Epoch 261) Total Loss: 9.064, KLD Loss: 1.690, MSE Loss: 7.374\n",
      "(Epoch 262) Total Loss: 8.300, KLD Loss: 1.685, MSE Loss: 6.615\n",
      "(Epoch 263) Total Loss: 6.605, KLD Loss: 1.543, MSE Loss: 5.061\n",
      "(Epoch 264) Total Loss: 9.472, KLD Loss: 1.425, MSE Loss: 8.046\n",
      "(Epoch 265) Total Loss: 11.028, KLD Loss: 1.482, MSE Loss: 9.545\n",
      "(Epoch 266) Total Loss: 6.021, KLD Loss: 1.708, MSE Loss: 4.313\n",
      "(Epoch 267) Total Loss: 7.452, KLD Loss: 1.635, MSE Loss: 5.817\n",
      "(Epoch 268) Total Loss: 12.030, KLD Loss: 1.718, MSE Loss: 10.312\n",
      "(Epoch 269) Total Loss: 22.310, KLD Loss: 1.602, MSE Loss: 20.708\n",
      "(Epoch 270) Total Loss: 22.926, KLD Loss: 1.260, MSE Loss: 21.666\n",
      "(Epoch 271) Total Loss: 7.043, KLD Loss: 1.778, MSE Loss: 5.266\n",
      "(Epoch 272) Total Loss: 21.507, KLD Loss: 1.559, MSE Loss: 19.948\n",
      "(Epoch 273) Total Loss: 8.131, KLD Loss: 1.531, MSE Loss: 6.600\n",
      "(Epoch 274) Total Loss: 7.220, KLD Loss: 1.417, MSE Loss: 5.802\n",
      "(Epoch 275) Total Loss: 5.013, KLD Loss: 1.645, MSE Loss: 3.368\n",
      "(Epoch 276) Total Loss: 8.218, KLD Loss: 1.600, MSE Loss: 6.618\n",
      "(Epoch 277) Total Loss: 5.744, KLD Loss: 1.681, MSE Loss: 4.063\n",
      "(Epoch 278) Total Loss: 7.821, KLD Loss: 1.620, MSE Loss: 6.200\n",
      "(Epoch 279) Total Loss: 3.166, KLD Loss: 1.583, MSE Loss: 1.583\n",
      "(Epoch 280) Total Loss: 6.262, KLD Loss: 1.871, MSE Loss: 4.391\n",
      "(Epoch 281) Total Loss: 19.241, KLD Loss: 1.611, MSE Loss: 17.630\n",
      "(Epoch 282) Total Loss: 9.792, KLD Loss: 1.507, MSE Loss: 8.285\n",
      "(Epoch 283) Total Loss: 8.268, KLD Loss: 1.589, MSE Loss: 6.679\n",
      "(Epoch 284) Total Loss: 3.029, KLD Loss: 1.471, MSE Loss: 1.558\n",
      "(Epoch 285) Total Loss: 6.119, KLD Loss: 1.622, MSE Loss: 4.498\n",
      "(Epoch 286) Total Loss: 6.956, KLD Loss: 1.507, MSE Loss: 5.449\n",
      "(Epoch 287) Total Loss: 11.799, KLD Loss: 1.540, MSE Loss: 10.259\n",
      "(Epoch 288) Total Loss: 20.393, KLD Loss: 1.420, MSE Loss: 18.973\n",
      "(Epoch 289) Total Loss: 9.213, KLD Loss: 1.511, MSE Loss: 7.702\n",
      "(Epoch 290) Total Loss: 7.517, KLD Loss: 1.676, MSE Loss: 5.840\n",
      "(Epoch 291) Total Loss: 5.157, KLD Loss: 1.563, MSE Loss: 3.594\n",
      "(Epoch 292) Total Loss: 7.424, KLD Loss: 1.802, MSE Loss: 5.623\n",
      "(Epoch 293) Total Loss: 6.115, KLD Loss: 1.430, MSE Loss: 4.685\n",
      "(Epoch 294) Total Loss: 9.993, KLD Loss: 1.960, MSE Loss: 8.033\n",
      "(Epoch 295) Total Loss: 7.414, KLD Loss: 1.427, MSE Loss: 5.988\n",
      "(Epoch 296) Total Loss: 5.363, KLD Loss: 1.700, MSE Loss: 3.663\n",
      "(Epoch 297) Total Loss: 12.424, KLD Loss: 1.681, MSE Loss: 10.743\n",
      "(Epoch 298) Total Loss: 21.941, KLD Loss: 1.669, MSE Loss: 20.272\n",
      "(Epoch 299) Total Loss: 8.687, KLD Loss: 1.603, MSE Loss: 7.084\n",
      "(Epoch 300) Total Loss: 4.458, KLD Loss: 1.599, MSE Loss: 2.859\n",
      "(Epoch 301) Total Loss: 5.383, KLD Loss: 1.631, MSE Loss: 3.752\n",
      "(Epoch 302) Total Loss: 7.807, KLD Loss: 1.588, MSE Loss: 6.219\n",
      "(Epoch 303) Total Loss: 17.775, KLD Loss: 1.447, MSE Loss: 16.328\n",
      "(Epoch 304) Total Loss: 22.270, KLD Loss: 1.569, MSE Loss: 20.701\n",
      "(Epoch 305) Total Loss: 17.532, KLD Loss: 1.591, MSE Loss: 15.941\n",
      "(Epoch 306) Total Loss: 5.613, KLD Loss: 1.840, MSE Loss: 3.773\n",
      "(Epoch 307) Total Loss: 5.464, KLD Loss: 1.704, MSE Loss: 3.760\n",
      "(Epoch 308) Total Loss: 9.165, KLD Loss: 1.597, MSE Loss: 7.568\n",
      "(Epoch 309) Total Loss: 7.116, KLD Loss: 1.618, MSE Loss: 5.498\n",
      "(Epoch 310) Total Loss: 21.976, KLD Loss: 1.954, MSE Loss: 20.022\n",
      "(Epoch 311) Total Loss: 5.657, KLD Loss: 1.282, MSE Loss: 4.375\n",
      "(Epoch 312) Total Loss: 16.434, KLD Loss: 1.577, MSE Loss: 14.857\n",
      "(Epoch 313) Total Loss: 6.326, KLD Loss: 1.488, MSE Loss: 4.837\n",
      "(Epoch 314) Total Loss: 5.526, KLD Loss: 1.762, MSE Loss: 3.764\n",
      "(Epoch 315) Total Loss: 20.321, KLD Loss: 1.519, MSE Loss: 18.802\n",
      "(Epoch 316) Total Loss: 15.324, KLD Loss: 1.478, MSE Loss: 13.846\n",
      "(Epoch 317) Total Loss: 9.683, KLD Loss: 1.639, MSE Loss: 8.043\n",
      "(Epoch 318) Total Loss: 6.540, KLD Loss: 1.538, MSE Loss: 5.002\n",
      "(Epoch 319) Total Loss: 31.505, KLD Loss: 1.736, MSE Loss: 29.769\n",
      "(Epoch 320) Total Loss: 4.592, KLD Loss: 1.895, MSE Loss: 2.696\n",
      "(Epoch 321) Total Loss: 7.109, KLD Loss: 1.499, MSE Loss: 5.610\n",
      "(Epoch 322) Total Loss: 6.241, KLD Loss: 1.521, MSE Loss: 4.720\n",
      "(Epoch 323) Total Loss: 7.887, KLD Loss: 1.800, MSE Loss: 6.088\n",
      "(Epoch 324) Total Loss: 4.052, KLD Loss: 1.558, MSE Loss: 2.494\n",
      "(Epoch 325) Total Loss: 16.869, KLD Loss: 1.924, MSE Loss: 14.946\n",
      "(Epoch 326) Total Loss: 7.321, KLD Loss: 1.530, MSE Loss: 5.792\n",
      "(Epoch 327) Total Loss: 7.474, KLD Loss: 1.614, MSE Loss: 5.859\n",
      "(Epoch 328) Total Loss: 4.104, KLD Loss: 1.712, MSE Loss: 2.393\n",
      "(Epoch 329) Total Loss: 6.143, KLD Loss: 1.647, MSE Loss: 4.496\n",
      "(Epoch 330) Total Loss: 5.435, KLD Loss: 1.787, MSE Loss: 3.649\n",
      "(Epoch 331) Total Loss: 6.040, KLD Loss: 1.579, MSE Loss: 4.461\n",
      "(Epoch 332) Total Loss: 15.010, KLD Loss: 2.072, MSE Loss: 12.938\n",
      "(Epoch 333) Total Loss: 21.033, KLD Loss: 1.462, MSE Loss: 19.571\n",
      "(Epoch 334) Total Loss: 6.929, KLD Loss: 1.498, MSE Loss: 5.431\n",
      "(Epoch 335) Total Loss: 11.757, KLD Loss: 1.668, MSE Loss: 10.090\n",
      "(Epoch 336) Total Loss: 20.306, KLD Loss: 1.652, MSE Loss: 18.654\n",
      "(Epoch 337) Total Loss: 7.651, KLD Loss: 1.499, MSE Loss: 6.152\n",
      "(Epoch 338) Total Loss: 6.224, KLD Loss: 1.629, MSE Loss: 4.595\n",
      "(Epoch 339) Total Loss: 3.539, KLD Loss: 1.668, MSE Loss: 1.871\n",
      "(Epoch 340) Total Loss: 5.311, KLD Loss: 1.672, MSE Loss: 3.640\n",
      "(Epoch 341) Total Loss: 7.552, KLD Loss: 1.681, MSE Loss: 5.872\n",
      "(Epoch 342) Total Loss: 4.237, KLD Loss: 1.722, MSE Loss: 2.514\n",
      "(Epoch 343) Total Loss: 10.435, KLD Loss: 1.585, MSE Loss: 8.851\n",
      "(Epoch 344) Total Loss: 4.377, KLD Loss: 1.544, MSE Loss: 2.833\n",
      "(Epoch 345) Total Loss: 18.356, KLD Loss: 1.522, MSE Loss: 16.833\n",
      "(Epoch 346) Total Loss: 7.307, KLD Loss: 1.642, MSE Loss: 5.665\n",
      "(Epoch 347) Total Loss: 4.138, KLD Loss: 1.875, MSE Loss: 2.263\n",
      "(Epoch 348) Total Loss: 15.244, KLD Loss: 1.776, MSE Loss: 13.468\n",
      "(Epoch 349) Total Loss: 27.170, KLD Loss: 1.591, MSE Loss: 25.579\n",
      "(Epoch 350) Total Loss: 5.694, KLD Loss: 1.836, MSE Loss: 3.858\n",
      "(Epoch 351) Total Loss: 32.043, KLD Loss: 1.811, MSE Loss: 30.232\n",
      "(Epoch 352) Total Loss: 17.219, KLD Loss: 1.949, MSE Loss: 15.270\n",
      "(Epoch 353) Total Loss: 10.297, KLD Loss: 1.481, MSE Loss: 8.817\n",
      "(Epoch 354) Total Loss: 4.690, KLD Loss: 1.675, MSE Loss: 3.015\n",
      "(Epoch 355) Total Loss: 8.800, KLD Loss: 1.506, MSE Loss: 7.294\n",
      "(Epoch 356) Total Loss: 7.182, KLD Loss: 1.745, MSE Loss: 5.437\n",
      "(Epoch 357) Total Loss: 9.501, KLD Loss: 1.669, MSE Loss: 7.832\n",
      "(Epoch 358) Total Loss: 7.786, KLD Loss: 1.507, MSE Loss: 6.280\n",
      "(Epoch 359) Total Loss: 24.319, KLD Loss: 1.593, MSE Loss: 22.726\n",
      "(Epoch 360) Total Loss: 3.410, KLD Loss: 1.567, MSE Loss: 1.844\n",
      "(Epoch 361) Total Loss: 4.811, KLD Loss: 1.308, MSE Loss: 3.503\n",
      "(Epoch 362) Total Loss: 28.159, KLD Loss: 1.752, MSE Loss: 26.408\n",
      "(Epoch 363) Total Loss: 9.186, KLD Loss: 1.715, MSE Loss: 7.471\n",
      "(Epoch 364) Total Loss: 10.221, KLD Loss: 1.924, MSE Loss: 8.297\n",
      "(Epoch 365) Total Loss: 6.801, KLD Loss: 1.697, MSE Loss: 5.104\n",
      "(Epoch 366) Total Loss: 5.141, KLD Loss: 1.772, MSE Loss: 3.369\n",
      "(Epoch 367) Total Loss: 7.427, KLD Loss: 1.582, MSE Loss: 5.844\n",
      "(Epoch 368) Total Loss: 4.926, KLD Loss: 1.505, MSE Loss: 3.422\n",
      "(Epoch 369) Total Loss: 20.046, KLD Loss: 1.596, MSE Loss: 18.450\n",
      "(Epoch 370) Total Loss: 5.736, KLD Loss: 1.797, MSE Loss: 3.939\n",
      "(Epoch 371) Total Loss: 6.319, KLD Loss: 1.630, MSE Loss: 4.689\n",
      "(Epoch 372) Total Loss: 5.842, KLD Loss: 1.654, MSE Loss: 4.187\n",
      "(Epoch 373) Total Loss: 4.378, KLD Loss: 1.674, MSE Loss: 2.703\n",
      "(Epoch 374) Total Loss: 17.464, KLD Loss: 1.832, MSE Loss: 15.632\n",
      "(Epoch 375) Total Loss: 9.654, KLD Loss: 1.523, MSE Loss: 8.130\n",
      "(Epoch 376) Total Loss: 7.715, KLD Loss: 1.837, MSE Loss: 5.878\n",
      "(Epoch 377) Total Loss: 4.506, KLD Loss: 1.587, MSE Loss: 2.919\n",
      "(Epoch 378) Total Loss: 19.074, KLD Loss: 1.446, MSE Loss: 17.628\n",
      "(Epoch 379) Total Loss: 10.794, KLD Loss: 1.440, MSE Loss: 9.354\n",
      "(Epoch 380) Total Loss: 19.355, KLD Loss: 1.535, MSE Loss: 17.820\n",
      "(Epoch 381) Total Loss: 9.640, KLD Loss: 1.720, MSE Loss: 7.921\n",
      "(Epoch 382) Total Loss: 10.144, KLD Loss: 1.443, MSE Loss: 8.702\n",
      "(Epoch 383) Total Loss: 5.754, KLD Loss: 1.676, MSE Loss: 4.079\n",
      "(Epoch 384) Total Loss: 16.238, KLD Loss: 1.623, MSE Loss: 14.616\n",
      "(Epoch 385) Total Loss: 9.526, KLD Loss: 1.816, MSE Loss: 7.710\n",
      "(Epoch 386) Total Loss: 5.393, KLD Loss: 1.663, MSE Loss: 3.730\n",
      "(Epoch 387) Total Loss: 7.906, KLD Loss: 1.888, MSE Loss: 6.019\n",
      "(Epoch 388) Total Loss: 6.639, KLD Loss: 1.565, MSE Loss: 5.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 389) Total Loss: 11.664, KLD Loss: 1.548, MSE Loss: 10.116\n",
      "(Epoch 390) Total Loss: 6.301, KLD Loss: 1.538, MSE Loss: 4.763\n",
      "(Epoch 391) Total Loss: 8.360, KLD Loss: 1.683, MSE Loss: 6.677\n",
      "(Epoch 392) Total Loss: 6.747, KLD Loss: 1.582, MSE Loss: 5.165\n",
      "(Epoch 393) Total Loss: 6.917, KLD Loss: 1.671, MSE Loss: 5.246\n",
      "(Epoch 394) Total Loss: 5.077, KLD Loss: 1.820, MSE Loss: 3.257\n",
      "(Epoch 395) Total Loss: 10.036, KLD Loss: 1.739, MSE Loss: 8.297\n",
      "(Epoch 396) Total Loss: 5.430, KLD Loss: 1.697, MSE Loss: 3.733\n",
      "(Epoch 397) Total Loss: 5.799, KLD Loss: 1.519, MSE Loss: 4.280\n",
      "(Epoch 398) Total Loss: 6.581, KLD Loss: 1.616, MSE Loss: 4.965\n",
      "(Epoch 399) Total Loss: 5.111, KLD Loss: 1.848, MSE Loss: 3.263\n",
      "(Epoch 400) Total Loss: 11.884, KLD Loss: 1.737, MSE Loss: 10.147\n",
      "(Epoch 401) Total Loss: 7.778, KLD Loss: 1.733, MSE Loss: 6.045\n",
      "(Epoch 402) Total Loss: 6.380, KLD Loss: 1.616, MSE Loss: 4.764\n",
      "(Epoch 403) Total Loss: 8.918, KLD Loss: 1.656, MSE Loss: 7.262\n",
      "(Epoch 404) Total Loss: 8.463, KLD Loss: 1.795, MSE Loss: 6.668\n",
      "(Epoch 405) Total Loss: 5.802, KLD Loss: 1.726, MSE Loss: 4.076\n",
      "(Epoch 406) Total Loss: 25.149, KLD Loss: 1.740, MSE Loss: 23.408\n",
      "(Epoch 407) Total Loss: 8.332, KLD Loss: 1.588, MSE Loss: 6.744\n",
      "(Epoch 408) Total Loss: 4.973, KLD Loss: 1.542, MSE Loss: 3.431\n",
      "(Epoch 409) Total Loss: 4.936, KLD Loss: 1.610, MSE Loss: 3.326\n",
      "(Epoch 410) Total Loss: 4.315, KLD Loss: 1.712, MSE Loss: 2.603\n",
      "(Epoch 411) Total Loss: 4.087, KLD Loss: 1.615, MSE Loss: 2.472\n",
      "(Epoch 412) Total Loss: 3.678, KLD Loss: 1.545, MSE Loss: 2.132\n",
      "(Epoch 413) Total Loss: 5.123, KLD Loss: 1.753, MSE Loss: 3.370\n",
      "(Epoch 414) Total Loss: 3.241, KLD Loss: 1.654, MSE Loss: 1.588\n",
      "(Epoch 415) Total Loss: 4.027, KLD Loss: 1.559, MSE Loss: 2.469\n",
      "(Epoch 416) Total Loss: 8.947, KLD Loss: 1.758, MSE Loss: 7.188\n",
      "(Epoch 417) Total Loss: 7.809, KLD Loss: 1.397, MSE Loss: 6.412\n",
      "(Epoch 418) Total Loss: 5.037, KLD Loss: 1.532, MSE Loss: 3.506\n",
      "(Epoch 419) Total Loss: 15.232, KLD Loss: 1.783, MSE Loss: 13.449\n",
      "(Epoch 420) Total Loss: 8.429, KLD Loss: 1.527, MSE Loss: 6.902\n",
      "(Epoch 421) Total Loss: 5.911, KLD Loss: 1.828, MSE Loss: 4.083\n",
      "(Epoch 422) Total Loss: 14.356, KLD Loss: 1.861, MSE Loss: 12.494\n",
      "(Epoch 423) Total Loss: 6.367, KLD Loss: 1.619, MSE Loss: 4.748\n",
      "(Epoch 424) Total Loss: 8.169, KLD Loss: 1.522, MSE Loss: 6.647\n",
      "(Epoch 425) Total Loss: 6.714, KLD Loss: 1.820, MSE Loss: 4.895\n",
      "(Epoch 426) Total Loss: 5.620, KLD Loss: 2.190, MSE Loss: 3.430\n",
      "(Epoch 427) Total Loss: 4.131, KLD Loss: 1.582, MSE Loss: 2.549\n",
      "(Epoch 428) Total Loss: 7.202, KLD Loss: 1.661, MSE Loss: 5.541\n",
      "(Epoch 429) Total Loss: 8.213, KLD Loss: 1.714, MSE Loss: 6.499\n",
      "(Epoch 430) Total Loss: 9.148, KLD Loss: 1.621, MSE Loss: 7.527\n",
      "(Epoch 431) Total Loss: 5.524, KLD Loss: 1.660, MSE Loss: 3.864\n",
      "(Epoch 432) Total Loss: 6.164, KLD Loss: 1.762, MSE Loss: 4.402\n",
      "(Epoch 433) Total Loss: 30.733, KLD Loss: 1.298, MSE Loss: 29.435\n",
      "(Epoch 434) Total Loss: 7.305, KLD Loss: 1.775, MSE Loss: 5.531\n",
      "(Epoch 435) Total Loss: 12.010, KLD Loss: 1.781, MSE Loss: 10.229\n",
      "(Epoch 436) Total Loss: 6.607, KLD Loss: 1.669, MSE Loss: 4.938\n",
      "(Epoch 437) Total Loss: 6.810, KLD Loss: 1.862, MSE Loss: 4.948\n",
      "(Epoch 438) Total Loss: 5.921, KLD Loss: 1.514, MSE Loss: 4.406\n",
      "(Epoch 439) Total Loss: 5.383, KLD Loss: 1.736, MSE Loss: 3.647\n",
      "(Epoch 440) Total Loss: 5.438, KLD Loss: 1.647, MSE Loss: 3.791\n",
      "(Epoch 441) Total Loss: 8.101, KLD Loss: 1.584, MSE Loss: 6.517\n",
      "(Epoch 442) Total Loss: 4.962, KLD Loss: 1.727, MSE Loss: 3.235\n",
      "(Epoch 443) Total Loss: 10.255, KLD Loss: 1.704, MSE Loss: 8.551\n",
      "(Epoch 444) Total Loss: 4.928, KLD Loss: 1.488, MSE Loss: 3.440\n",
      "(Epoch 445) Total Loss: 12.345, KLD Loss: 1.514, MSE Loss: 10.830\n",
      "(Epoch 446) Total Loss: 21.166, KLD Loss: 1.644, MSE Loss: 19.521\n",
      "(Epoch 447) Total Loss: 5.553, KLD Loss: 1.780, MSE Loss: 3.773\n",
      "(Epoch 448) Total Loss: 18.004, KLD Loss: 1.753, MSE Loss: 16.251\n",
      "(Epoch 449) Total Loss: 5.198, KLD Loss: 1.558, MSE Loss: 3.640\n",
      "(Epoch 450) Total Loss: 5.129, KLD Loss: 1.788, MSE Loss: 3.341\n",
      "(Epoch 451) Total Loss: 5.711, KLD Loss: 1.794, MSE Loss: 3.918\n",
      "(Epoch 452) Total Loss: 7.360, KLD Loss: 1.448, MSE Loss: 5.913\n",
      "(Epoch 453) Total Loss: 9.771, KLD Loss: 1.840, MSE Loss: 7.931\n",
      "(Epoch 454) Total Loss: 7.571, KLD Loss: 1.642, MSE Loss: 5.930\n",
      "(Epoch 455) Total Loss: 18.714, KLD Loss: 1.629, MSE Loss: 17.085\n",
      "(Epoch 456) Total Loss: 6.025, KLD Loss: 1.615, MSE Loss: 4.410\n",
      "(Epoch 457) Total Loss: 5.682, KLD Loss: 1.586, MSE Loss: 4.096\n",
      "(Epoch 458) Total Loss: 8.027, KLD Loss: 1.643, MSE Loss: 6.383\n",
      "(Epoch 459) Total Loss: 15.973, KLD Loss: 1.497, MSE Loss: 14.475\n",
      "(Epoch 460) Total Loss: 6.631, KLD Loss: 1.620, MSE Loss: 5.011\n",
      "(Epoch 461) Total Loss: 9.777, KLD Loss: 1.833, MSE Loss: 7.944\n",
      "(Epoch 462) Total Loss: 6.330, KLD Loss: 1.619, MSE Loss: 4.711\n",
      "(Epoch 463) Total Loss: 9.377, KLD Loss: 1.712, MSE Loss: 7.665\n",
      "(Epoch 464) Total Loss: 10.927, KLD Loss: 1.587, MSE Loss: 9.340\n",
      "(Epoch 465) Total Loss: 10.882, KLD Loss: 1.462, MSE Loss: 9.420\n",
      "(Epoch 466) Total Loss: 11.702, KLD Loss: 1.661, MSE Loss: 10.041\n",
      "(Epoch 467) Total Loss: 3.755, KLD Loss: 1.716, MSE Loss: 2.038\n",
      "(Epoch 468) Total Loss: 2.834, KLD Loss: 1.588, MSE Loss: 1.245\n",
      "(Epoch 469) Total Loss: 7.653, KLD Loss: 1.677, MSE Loss: 5.975\n",
      "(Epoch 470) Total Loss: 6.511, KLD Loss: 1.702, MSE Loss: 4.809\n",
      "(Epoch 471) Total Loss: 4.916, KLD Loss: 1.705, MSE Loss: 3.211\n",
      "(Epoch 472) Total Loss: 8.024, KLD Loss: 1.729, MSE Loss: 6.295\n",
      "(Epoch 473) Total Loss: 18.519, KLD Loss: 1.751, MSE Loss: 16.768\n",
      "(Epoch 474) Total Loss: 6.214, KLD Loss: 1.626, MSE Loss: 4.589\n",
      "(Epoch 475) Total Loss: 13.478, KLD Loss: 1.542, MSE Loss: 11.936\n",
      "(Epoch 476) Total Loss: 4.522, KLD Loss: 1.815, MSE Loss: 2.708\n",
      "(Epoch 477) Total Loss: 3.481, KLD Loss: 1.430, MSE Loss: 2.051\n",
      "(Epoch 478) Total Loss: 3.946, KLD Loss: 1.487, MSE Loss: 2.458\n",
      "(Epoch 479) Total Loss: 5.190, KLD Loss: 1.536, MSE Loss: 3.654\n",
      "(Epoch 480) Total Loss: 7.721, KLD Loss: 1.710, MSE Loss: 6.011\n",
      "(Epoch 481) Total Loss: 6.072, KLD Loss: 1.472, MSE Loss: 4.600\n",
      "(Epoch 482) Total Loss: 4.443, KLD Loss: 1.494, MSE Loss: 2.950\n",
      "(Epoch 483) Total Loss: 9.009, KLD Loss: 1.629, MSE Loss: 7.381\n",
      "(Epoch 484) Total Loss: 7.689, KLD Loss: 1.562, MSE Loss: 6.126\n",
      "(Epoch 485) Total Loss: 6.363, KLD Loss: 1.670, MSE Loss: 4.693\n",
      "(Epoch 486) Total Loss: 8.346, KLD Loss: 1.606, MSE Loss: 6.741\n",
      "(Epoch 487) Total Loss: 3.400, KLD Loss: 1.695, MSE Loss: 1.704\n",
      "(Epoch 488) Total Loss: 3.451, KLD Loss: 1.794, MSE Loss: 1.657\n",
      "(Epoch 489) Total Loss: 20.997, KLD Loss: 1.485, MSE Loss: 19.511\n",
      "(Epoch 490) Total Loss: 5.103, KLD Loss: 1.584, MSE Loss: 3.518\n",
      "(Epoch 491) Total Loss: 4.652, KLD Loss: 1.438, MSE Loss: 3.214\n",
      "(Epoch 492) Total Loss: 7.633, KLD Loss: 1.554, MSE Loss: 6.080\n",
      "(Epoch 493) Total Loss: 24.137, KLD Loss: 1.620, MSE Loss: 22.517\n",
      "(Epoch 494) Total Loss: 4.153, KLD Loss: 1.655, MSE Loss: 2.498\n",
      "(Epoch 495) Total Loss: 5.011, KLD Loss: 1.619, MSE Loss: 3.392\n",
      "(Epoch 496) Total Loss: 6.761, KLD Loss: 1.566, MSE Loss: 5.195\n",
      "(Epoch 497) Total Loss: 8.991, KLD Loss: 1.549, MSE Loss: 7.442\n",
      "(Epoch 498) Total Loss: 6.865, KLD Loss: 1.713, MSE Loss: 5.153\n",
      "(Epoch 499) Total Loss: 2.559, KLD Loss: 1.734, MSE Loss: 0.825\n",
      "(Epoch 500) Total Loss: 16.815, KLD Loss: 1.604, MSE Loss: 15.212\n",
      "(Epoch 501) Total Loss: 11.400, KLD Loss: 1.891, MSE Loss: 9.509\n",
      "(Epoch 502) Total Loss: 4.138, KLD Loss: 1.611, MSE Loss: 2.527\n",
      "(Epoch 503) Total Loss: 13.970, KLD Loss: 1.713, MSE Loss: 12.257\n",
      "(Epoch 504) Total Loss: 5.031, KLD Loss: 1.360, MSE Loss: 3.671\n",
      "(Epoch 505) Total Loss: 6.610, KLD Loss: 1.827, MSE Loss: 4.783\n",
      "(Epoch 506) Total Loss: 7.519, KLD Loss: 1.767, MSE Loss: 5.752\n",
      "(Epoch 507) Total Loss: 11.098, KLD Loss: 1.863, MSE Loss: 9.235\n",
      "(Epoch 508) Total Loss: 5.181, KLD Loss: 1.548, MSE Loss: 3.634\n",
      "(Epoch 509) Total Loss: 20.704, KLD Loss: 1.518, MSE Loss: 19.186\n",
      "(Epoch 510) Total Loss: 16.581, KLD Loss: 1.680, MSE Loss: 14.901\n",
      "(Epoch 511) Total Loss: 8.685, KLD Loss: 1.739, MSE Loss: 6.946\n",
      "(Epoch 512) Total Loss: 4.265, KLD Loss: 1.586, MSE Loss: 2.679\n",
      "(Epoch 513) Total Loss: 23.104, KLD Loss: 2.018, MSE Loss: 21.086\n",
      "(Epoch 514) Total Loss: 4.240, KLD Loss: 1.603, MSE Loss: 2.638\n",
      "(Epoch 515) Total Loss: 12.785, KLD Loss: 1.639, MSE Loss: 11.147\n",
      "(Epoch 516) Total Loss: 10.014, KLD Loss: 1.748, MSE Loss: 8.266\n",
      "(Epoch 517) Total Loss: 4.835, KLD Loss: 1.633, MSE Loss: 3.202\n",
      "(Epoch 518) Total Loss: 8.456, KLD Loss: 1.365, MSE Loss: 7.091\n",
      "(Epoch 519) Total Loss: 13.124, KLD Loss: 1.511, MSE Loss: 11.613\n",
      "(Epoch 520) Total Loss: 5.934, KLD Loss: 1.528, MSE Loss: 4.406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 521) Total Loss: 7.115, KLD Loss: 1.711, MSE Loss: 5.404\n",
      "(Epoch 522) Total Loss: 4.840, KLD Loss: 1.447, MSE Loss: 3.393\n",
      "(Epoch 523) Total Loss: 3.884, KLD Loss: 1.468, MSE Loss: 2.416\n",
      "(Epoch 524) Total Loss: 7.052, KLD Loss: 1.981, MSE Loss: 5.071\n",
      "(Epoch 525) Total Loss: 5.239, KLD Loss: 1.815, MSE Loss: 3.424\n",
      "(Epoch 526) Total Loss: 3.407, KLD Loss: 1.556, MSE Loss: 1.852\n",
      "(Epoch 527) Total Loss: 5.747, KLD Loss: 1.730, MSE Loss: 4.017\n",
      "(Epoch 528) Total Loss: 18.105, KLD Loss: 1.659, MSE Loss: 16.446\n",
      "(Epoch 529) Total Loss: 8.186, KLD Loss: 1.591, MSE Loss: 6.595\n",
      "(Epoch 530) Total Loss: 7.096, KLD Loss: 1.716, MSE Loss: 5.380\n",
      "(Epoch 531) Total Loss: 8.998, KLD Loss: 2.004, MSE Loss: 6.994\n",
      "(Epoch 532) Total Loss: 16.184, KLD Loss: 1.619, MSE Loss: 14.565\n",
      "(Epoch 533) Total Loss: 15.221, KLD Loss: 1.678, MSE Loss: 13.543\n",
      "(Epoch 534) Total Loss: 7.503, KLD Loss: 1.547, MSE Loss: 5.955\n",
      "(Epoch 535) Total Loss: 4.616, KLD Loss: 1.565, MSE Loss: 3.051\n",
      "(Epoch 536) Total Loss: 4.040, KLD Loss: 1.702, MSE Loss: 2.338\n",
      "(Epoch 537) Total Loss: 14.624, KLD Loss: 1.573, MSE Loss: 13.051\n",
      "(Epoch 538) Total Loss: 5.917, KLD Loss: 1.597, MSE Loss: 4.319\n",
      "(Epoch 539) Total Loss: 16.972, KLD Loss: 1.573, MSE Loss: 15.399\n",
      "(Epoch 540) Total Loss: 15.565, KLD Loss: 1.547, MSE Loss: 14.018\n",
      "(Epoch 541) Total Loss: 14.219, KLD Loss: 1.695, MSE Loss: 12.525\n",
      "(Epoch 542) Total Loss: 15.062, KLD Loss: 1.776, MSE Loss: 13.285\n",
      "(Epoch 543) Total Loss: 8.495, KLD Loss: 1.556, MSE Loss: 6.939\n",
      "(Epoch 544) Total Loss: 4.473, KLD Loss: 1.970, MSE Loss: 2.503\n",
      "(Epoch 545) Total Loss: 3.802, KLD Loss: 1.562, MSE Loss: 2.241\n",
      "(Epoch 546) Total Loss: 13.129, KLD Loss: 1.662, MSE Loss: 11.468\n",
      "(Epoch 547) Total Loss: 3.756, KLD Loss: 1.666, MSE Loss: 2.089\n",
      "(Epoch 548) Total Loss: 4.442, KLD Loss: 1.551, MSE Loss: 2.891\n",
      "(Epoch 549) Total Loss: 6.234, KLD Loss: 1.506, MSE Loss: 4.728\n",
      "(Epoch 550) Total Loss: 6.906, KLD Loss: 1.734, MSE Loss: 5.173\n",
      "(Epoch 551) Total Loss: 4.904, KLD Loss: 1.579, MSE Loss: 3.325\n",
      "(Epoch 552) Total Loss: 13.259, KLD Loss: 1.911, MSE Loss: 11.348\n",
      "(Epoch 553) Total Loss: 12.371, KLD Loss: 1.753, MSE Loss: 10.617\n",
      "(Epoch 554) Total Loss: 14.394, KLD Loss: 1.694, MSE Loss: 12.700\n",
      "(Epoch 555) Total Loss: 6.698, KLD Loss: 1.561, MSE Loss: 5.137\n",
      "(Epoch 556) Total Loss: 8.138, KLD Loss: 1.643, MSE Loss: 6.495\n",
      "(Epoch 557) Total Loss: 13.217, KLD Loss: 1.667, MSE Loss: 11.550\n",
      "(Epoch 558) Total Loss: 5.138, KLD Loss: 1.805, MSE Loss: 3.333\n",
      "(Epoch 559) Total Loss: 4.304, KLD Loss: 1.549, MSE Loss: 2.755\n",
      "(Epoch 560) Total Loss: 6.872, KLD Loss: 1.644, MSE Loss: 5.227\n",
      "(Epoch 561) Total Loss: 11.504, KLD Loss: 1.531, MSE Loss: 9.973\n",
      "(Epoch 562) Total Loss: 9.910, KLD Loss: 1.519, MSE Loss: 8.391\n",
      "(Epoch 563) Total Loss: 3.852, KLD Loss: 1.605, MSE Loss: 2.247\n",
      "(Epoch 564) Total Loss: 4.125, KLD Loss: 1.818, MSE Loss: 2.307\n",
      "(Epoch 565) Total Loss: 17.315, KLD Loss: 2.057, MSE Loss: 15.258\n",
      "(Epoch 566) Total Loss: 7.381, KLD Loss: 1.644, MSE Loss: 5.737\n",
      "(Epoch 567) Total Loss: 4.481, KLD Loss: 1.462, MSE Loss: 3.019\n",
      "(Epoch 568) Total Loss: 3.819, KLD Loss: 1.595, MSE Loss: 2.224\n",
      "(Epoch 569) Total Loss: 8.524, KLD Loss: 1.848, MSE Loss: 6.676\n",
      "(Epoch 570) Total Loss: 4.065, KLD Loss: 1.591, MSE Loss: 2.474\n",
      "(Epoch 571) Total Loss: 9.969, KLD Loss: 1.531, MSE Loss: 8.438\n",
      "(Epoch 572) Total Loss: 3.380, KLD Loss: 1.460, MSE Loss: 1.920\n",
      "(Epoch 573) Total Loss: 14.566, KLD Loss: 1.882, MSE Loss: 12.684\n",
      "(Epoch 574) Total Loss: 8.927, KLD Loss: 1.723, MSE Loss: 7.204\n",
      "(Epoch 575) Total Loss: 5.023, KLD Loss: 1.492, MSE Loss: 3.531\n",
      "(Epoch 576) Total Loss: 21.307, KLD Loss: 1.778, MSE Loss: 19.530\n",
      "(Epoch 577) Total Loss: 10.712, KLD Loss: 1.719, MSE Loss: 8.993\n",
      "(Epoch 578) Total Loss: 5.151, KLD Loss: 1.766, MSE Loss: 3.385\n",
      "(Epoch 579) Total Loss: 3.224, KLD Loss: 1.586, MSE Loss: 1.638\n",
      "(Epoch 580) Total Loss: 7.059, KLD Loss: 1.456, MSE Loss: 5.603\n",
      "(Epoch 581) Total Loss: 6.083, KLD Loss: 1.530, MSE Loss: 4.553\n",
      "(Epoch 582) Total Loss: 8.397, KLD Loss: 1.732, MSE Loss: 6.665\n",
      "(Epoch 583) Total Loss: 6.899, KLD Loss: 1.561, MSE Loss: 5.339\n",
      "(Epoch 584) Total Loss: 6.644, KLD Loss: 1.839, MSE Loss: 4.806\n",
      "(Epoch 585) Total Loss: 7.750, KLD Loss: 1.433, MSE Loss: 6.318\n",
      "(Epoch 586) Total Loss: 8.709, KLD Loss: 1.609, MSE Loss: 7.101\n",
      "(Epoch 587) Total Loss: 16.361, KLD Loss: 1.739, MSE Loss: 14.622\n",
      "(Epoch 588) Total Loss: 20.412, KLD Loss: 1.538, MSE Loss: 18.874\n",
      "(Epoch 589) Total Loss: 7.288, KLD Loss: 1.612, MSE Loss: 5.676\n",
      "(Epoch 590) Total Loss: 6.909, KLD Loss: 1.722, MSE Loss: 5.186\n",
      "(Epoch 591) Total Loss: 6.171, KLD Loss: 1.658, MSE Loss: 4.513\n",
      "(Epoch 592) Total Loss: 5.574, KLD Loss: 1.612, MSE Loss: 3.962\n",
      "(Epoch 593) Total Loss: 15.038, KLD Loss: 1.574, MSE Loss: 13.464\n",
      "(Epoch 594) Total Loss: 4.456, KLD Loss: 1.795, MSE Loss: 2.660\n",
      "(Epoch 595) Total Loss: 7.328, KLD Loss: 1.798, MSE Loss: 5.530\n",
      "(Epoch 596) Total Loss: 4.744, KLD Loss: 1.863, MSE Loss: 2.881\n",
      "(Epoch 597) Total Loss: 3.988, KLD Loss: 1.541, MSE Loss: 2.446\n",
      "(Epoch 598) Total Loss: 9.125, KLD Loss: 1.932, MSE Loss: 7.193\n",
      "(Epoch 599) Total Loss: 8.937, KLD Loss: 1.873, MSE Loss: 7.064\n",
      "(Epoch 600) Total Loss: 5.557, KLD Loss: 1.674, MSE Loss: 3.883\n",
      "(Epoch 601) Total Loss: 10.428, KLD Loss: 1.804, MSE Loss: 8.623\n",
      "(Epoch 602) Total Loss: 5.307, KLD Loss: 1.840, MSE Loss: 3.466\n",
      "(Epoch 603) Total Loss: 9.466, KLD Loss: 2.118, MSE Loss: 7.349\n",
      "(Epoch 604) Total Loss: 6.276, KLD Loss: 1.685, MSE Loss: 4.590\n",
      "(Epoch 605) Total Loss: 7.991, KLD Loss: 1.609, MSE Loss: 6.382\n",
      "(Epoch 606) Total Loss: 16.272, KLD Loss: 1.943, MSE Loss: 14.329\n",
      "(Epoch 607) Total Loss: 6.884, KLD Loss: 1.715, MSE Loss: 5.168\n",
      "(Epoch 608) Total Loss: 5.930, KLD Loss: 1.749, MSE Loss: 4.182\n",
      "(Epoch 609) Total Loss: 11.884, KLD Loss: 1.702, MSE Loss: 10.182\n",
      "(Epoch 610) Total Loss: 21.509, KLD Loss: 1.703, MSE Loss: 19.806\n",
      "(Epoch 611) Total Loss: 6.227, KLD Loss: 1.526, MSE Loss: 4.701\n",
      "(Epoch 612) Total Loss: 5.597, KLD Loss: 1.717, MSE Loss: 3.879\n",
      "(Epoch 613) Total Loss: 7.044, KLD Loss: 1.648, MSE Loss: 5.397\n",
      "(Epoch 614) Total Loss: 5.384, KLD Loss: 1.558, MSE Loss: 3.826\n",
      "(Epoch 615) Total Loss: 5.001, KLD Loss: 1.784, MSE Loss: 3.216\n",
      "(Epoch 616) Total Loss: 7.241, KLD Loss: 1.750, MSE Loss: 5.491\n",
      "(Epoch 617) Total Loss: 4.575, KLD Loss: 1.718, MSE Loss: 2.857\n",
      "(Epoch 618) Total Loss: 7.007, KLD Loss: 1.612, MSE Loss: 5.395\n",
      "(Epoch 619) Total Loss: 2.218, KLD Loss: 1.413, MSE Loss: 0.806\n",
      "(Epoch 620) Total Loss: 9.007, KLD Loss: 1.740, MSE Loss: 7.267\n",
      "(Epoch 621) Total Loss: 6.844, KLD Loss: 1.627, MSE Loss: 5.216\n",
      "(Epoch 622) Total Loss: 6.852, KLD Loss: 1.557, MSE Loss: 5.295\n",
      "(Epoch 623) Total Loss: 6.546, KLD Loss: 1.659, MSE Loss: 4.887\n",
      "(Epoch 624) Total Loss: 6.645, KLD Loss: 1.550, MSE Loss: 5.095\n",
      "(Epoch 625) Total Loss: 6.935, KLD Loss: 1.488, MSE Loss: 5.448\n",
      "(Epoch 626) Total Loss: 5.716, KLD Loss: 1.684, MSE Loss: 4.032\n",
      "(Epoch 627) Total Loss: 15.794, KLD Loss: 1.721, MSE Loss: 14.074\n",
      "(Epoch 628) Total Loss: 9.019, KLD Loss: 1.739, MSE Loss: 7.279\n",
      "(Epoch 629) Total Loss: 27.212, KLD Loss: 1.546, MSE Loss: 25.666\n",
      "(Epoch 630) Total Loss: 2.082, KLD Loss: 1.542, MSE Loss: 0.539\n",
      "(Epoch 631) Total Loss: 7.126, KLD Loss: 1.375, MSE Loss: 5.752\n",
      "(Epoch 632) Total Loss: 11.457, KLD Loss: 1.943, MSE Loss: 9.514\n",
      "(Epoch 633) Total Loss: 14.144, KLD Loss: 1.747, MSE Loss: 12.397\n",
      "(Epoch 634) Total Loss: 5.224, KLD Loss: 1.543, MSE Loss: 3.681\n",
      "(Epoch 635) Total Loss: 17.529, KLD Loss: 1.593, MSE Loss: 15.936\n",
      "(Epoch 636) Total Loss: 4.785, KLD Loss: 1.893, MSE Loss: 2.891\n",
      "(Epoch 637) Total Loss: 5.315, KLD Loss: 1.610, MSE Loss: 3.705\n",
      "(Epoch 638) Total Loss: 7.515, KLD Loss: 1.336, MSE Loss: 6.179\n",
      "(Epoch 639) Total Loss: 6.440, KLD Loss: 1.613, MSE Loss: 4.827\n",
      "(Epoch 640) Total Loss: 8.200, KLD Loss: 1.808, MSE Loss: 6.391\n",
      "(Epoch 641) Total Loss: 2.544, KLD Loss: 1.425, MSE Loss: 1.119\n",
      "(Epoch 642) Total Loss: 4.397, KLD Loss: 1.641, MSE Loss: 2.756\n",
      "(Epoch 643) Total Loss: 7.421, KLD Loss: 1.619, MSE Loss: 5.802\n",
      "(Epoch 644) Total Loss: 4.267, KLD Loss: 1.393, MSE Loss: 2.874\n",
      "(Epoch 645) Total Loss: 18.314, KLD Loss: 1.726, MSE Loss: 16.589\n",
      "(Epoch 646) Total Loss: 8.222, KLD Loss: 1.527, MSE Loss: 6.695\n",
      "(Epoch 647) Total Loss: 5.921, KLD Loss: 1.510, MSE Loss: 4.411\n",
      "(Epoch 648) Total Loss: 5.487, KLD Loss: 1.504, MSE Loss: 3.983\n",
      "(Epoch 649) Total Loss: 4.671, KLD Loss: 1.675, MSE Loss: 2.996\n",
      "(Epoch 650) Total Loss: 5.583, KLD Loss: 1.531, MSE Loss: 4.052\n",
      "(Epoch 651) Total Loss: 5.709, KLD Loss: 1.535, MSE Loss: 4.174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 652) Total Loss: 7.722, KLD Loss: 1.977, MSE Loss: 5.745\n",
      "(Epoch 653) Total Loss: 5.833, KLD Loss: 1.482, MSE Loss: 4.351\n",
      "(Epoch 654) Total Loss: 3.898, KLD Loss: 1.560, MSE Loss: 2.338\n",
      "(Epoch 655) Total Loss: 18.399, KLD Loss: 1.720, MSE Loss: 16.678\n",
      "(Epoch 656) Total Loss: 2.803, KLD Loss: 1.464, MSE Loss: 1.339\n",
      "(Epoch 657) Total Loss: 7.959, KLD Loss: 1.575, MSE Loss: 6.384\n",
      "(Epoch 658) Total Loss: 16.917, KLD Loss: 1.458, MSE Loss: 15.459\n",
      "(Epoch 659) Total Loss: 5.725, KLD Loss: 1.723, MSE Loss: 4.002\n",
      "(Epoch 660) Total Loss: 4.510, KLD Loss: 1.720, MSE Loss: 2.791\n",
      "(Epoch 661) Total Loss: 2.061, KLD Loss: 1.623, MSE Loss: 0.438\n",
      "(Epoch 662) Total Loss: 3.131, KLD Loss: 1.432, MSE Loss: 1.699\n",
      "(Epoch 663) Total Loss: 5.488, KLD Loss: 1.770, MSE Loss: 3.718\n",
      "(Epoch 664) Total Loss: 14.609, KLD Loss: 1.676, MSE Loss: 12.933\n",
      "(Epoch 665) Total Loss: 5.518, KLD Loss: 1.866, MSE Loss: 3.652\n",
      "(Epoch 666) Total Loss: 6.314, KLD Loss: 1.597, MSE Loss: 4.716\n",
      "(Epoch 667) Total Loss: 6.563, KLD Loss: 1.589, MSE Loss: 4.974\n",
      "(Epoch 668) Total Loss: 5.543, KLD Loss: 1.760, MSE Loss: 3.783\n",
      "(Epoch 669) Total Loss: 16.927, KLD Loss: 1.391, MSE Loss: 15.537\n",
      "(Epoch 670) Total Loss: 5.723, KLD Loss: 1.670, MSE Loss: 4.053\n",
      "(Epoch 671) Total Loss: 6.724, KLD Loss: 1.913, MSE Loss: 4.811\n",
      "(Epoch 672) Total Loss: 4.999, KLD Loss: 1.777, MSE Loss: 3.222\n",
      "(Epoch 673) Total Loss: 3.432, KLD Loss: 1.360, MSE Loss: 2.072\n",
      "(Epoch 674) Total Loss: 5.769, KLD Loss: 1.898, MSE Loss: 3.871\n",
      "(Epoch 675) Total Loss: 4.941, KLD Loss: 1.421, MSE Loss: 3.520\n",
      "(Epoch 676) Total Loss: 14.661, KLD Loss: 2.007, MSE Loss: 12.655\n",
      "(Epoch 677) Total Loss: 6.517, KLD Loss: 1.804, MSE Loss: 4.713\n",
      "(Epoch 678) Total Loss: 3.132, KLD Loss: 1.622, MSE Loss: 1.510\n",
      "(Epoch 679) Total Loss: 10.794, KLD Loss: 1.517, MSE Loss: 9.277\n",
      "(Epoch 680) Total Loss: 6.203, KLD Loss: 1.840, MSE Loss: 4.362\n",
      "(Epoch 681) Total Loss: 2.319, KLD Loss: 1.534, MSE Loss: 0.784\n",
      "(Epoch 682) Total Loss: 7.628, KLD Loss: 1.500, MSE Loss: 6.127\n",
      "(Epoch 683) Total Loss: 3.838, KLD Loss: 1.493, MSE Loss: 2.346\n",
      "(Epoch 684) Total Loss: 5.986, KLD Loss: 1.737, MSE Loss: 4.249\n",
      "(Epoch 685) Total Loss: 7.146, KLD Loss: 1.672, MSE Loss: 5.474\n",
      "(Epoch 686) Total Loss: 13.037, KLD Loss: 1.695, MSE Loss: 11.342\n",
      "(Epoch 687) Total Loss: 5.818, KLD Loss: 1.610, MSE Loss: 4.209\n",
      "(Epoch 688) Total Loss: 7.364, KLD Loss: 1.798, MSE Loss: 5.566\n",
      "(Epoch 689) Total Loss: 6.418, KLD Loss: 1.813, MSE Loss: 4.605\n",
      "(Epoch 690) Total Loss: 12.374, KLD Loss: 1.709, MSE Loss: 10.665\n",
      "(Epoch 691) Total Loss: 6.400, KLD Loss: 1.674, MSE Loss: 4.726\n",
      "(Epoch 692) Total Loss: 14.763, KLD Loss: 1.427, MSE Loss: 13.337\n",
      "(Epoch 693) Total Loss: 3.256, KLD Loss: 1.571, MSE Loss: 1.685\n",
      "(Epoch 694) Total Loss: 14.232, KLD Loss: 1.664, MSE Loss: 12.568\n",
      "(Epoch 695) Total Loss: 5.366, KLD Loss: 1.661, MSE Loss: 3.705\n",
      "(Epoch 696) Total Loss: 4.947, KLD Loss: 1.399, MSE Loss: 3.548\n",
      "(Epoch 697) Total Loss: 4.908, KLD Loss: 1.667, MSE Loss: 3.241\n",
      "(Epoch 698) Total Loss: 7.792, KLD Loss: 1.515, MSE Loss: 6.277\n",
      "(Epoch 699) Total Loss: 5.319, KLD Loss: 1.577, MSE Loss: 3.742\n",
      "(Epoch 700) Total Loss: 6.080, KLD Loss: 1.719, MSE Loss: 4.360\n",
      "(Epoch 701) Total Loss: 4.783, KLD Loss: 1.673, MSE Loss: 3.110\n",
      "(Epoch 702) Total Loss: 10.586, KLD Loss: 1.682, MSE Loss: 8.904\n",
      "(Epoch 703) Total Loss: 4.306, KLD Loss: 1.577, MSE Loss: 2.728\n",
      "(Epoch 704) Total Loss: 4.506, KLD Loss: 1.758, MSE Loss: 2.748\n",
      "(Epoch 705) Total Loss: 6.308, KLD Loss: 1.758, MSE Loss: 4.550\n",
      "(Epoch 706) Total Loss: 4.768, KLD Loss: 1.631, MSE Loss: 3.137\n",
      "(Epoch 707) Total Loss: 5.125, KLD Loss: 1.795, MSE Loss: 3.330\n",
      "(Epoch 708) Total Loss: 6.360, KLD Loss: 1.769, MSE Loss: 4.591\n",
      "(Epoch 709) Total Loss: 8.120, KLD Loss: 1.725, MSE Loss: 6.395\n",
      "(Epoch 710) Total Loss: 3.857, KLD Loss: 1.575, MSE Loss: 2.282\n",
      "(Epoch 711) Total Loss: 6.146, KLD Loss: 1.692, MSE Loss: 4.454\n",
      "(Epoch 712) Total Loss: 4.665, KLD Loss: 1.521, MSE Loss: 3.144\n",
      "(Epoch 713) Total Loss: 6.227, KLD Loss: 1.598, MSE Loss: 4.628\n",
      "(Epoch 714) Total Loss: 6.901, KLD Loss: 1.742, MSE Loss: 5.159\n",
      "(Epoch 715) Total Loss: 6.073, KLD Loss: 1.637, MSE Loss: 4.436\n",
      "(Epoch 716) Total Loss: 6.257, KLD Loss: 1.442, MSE Loss: 4.815\n",
      "(Epoch 717) Total Loss: 4.965, KLD Loss: 1.540, MSE Loss: 3.425\n",
      "(Epoch 718) Total Loss: 5.667, KLD Loss: 1.718, MSE Loss: 3.949\n",
      "(Epoch 719) Total Loss: 9.114, KLD Loss: 1.497, MSE Loss: 7.617\n",
      "(Epoch 720) Total Loss: 14.632, KLD Loss: 1.702, MSE Loss: 12.930\n",
      "(Epoch 721) Total Loss: 10.178, KLD Loss: 1.648, MSE Loss: 8.530\n",
      "(Epoch 722) Total Loss: 5.537, KLD Loss: 1.469, MSE Loss: 4.068\n",
      "(Epoch 723) Total Loss: 6.389, KLD Loss: 1.455, MSE Loss: 4.934\n",
      "(Epoch 724) Total Loss: 3.548, KLD Loss: 1.690, MSE Loss: 1.858\n",
      "(Epoch 725) Total Loss: 7.981, KLD Loss: 1.908, MSE Loss: 6.074\n",
      "(Epoch 726) Total Loss: 6.398, KLD Loss: 1.704, MSE Loss: 4.694\n",
      "(Epoch 727) Total Loss: 7.750, KLD Loss: 1.842, MSE Loss: 5.908\n",
      "(Epoch 728) Total Loss: 5.913, KLD Loss: 1.762, MSE Loss: 4.151\n",
      "(Epoch 729) Total Loss: 9.184, KLD Loss: 1.535, MSE Loss: 7.649\n",
      "(Epoch 730) Total Loss: 16.245, KLD Loss: 1.780, MSE Loss: 14.465\n",
      "(Epoch 731) Total Loss: 16.764, KLD Loss: 1.803, MSE Loss: 14.961\n",
      "(Epoch 732) Total Loss: 3.808, KLD Loss: 1.547, MSE Loss: 2.260\n",
      "(Epoch 733) Total Loss: 3.526, KLD Loss: 1.573, MSE Loss: 1.953\n",
      "(Epoch 734) Total Loss: 3.789, KLD Loss: 1.575, MSE Loss: 2.214\n",
      "(Epoch 735) Total Loss: 4.669, KLD Loss: 1.648, MSE Loss: 3.021\n",
      "(Epoch 736) Total Loss: 18.742, KLD Loss: 1.487, MSE Loss: 17.255\n",
      "(Epoch 737) Total Loss: 12.888, KLD Loss: 1.666, MSE Loss: 11.222\n",
      "(Epoch 738) Total Loss: 6.005, KLD Loss: 1.664, MSE Loss: 4.341\n",
      "(Epoch 739) Total Loss: 4.212, KLD Loss: 1.751, MSE Loss: 2.461\n",
      "(Epoch 740) Total Loss: 6.324, KLD Loss: 1.803, MSE Loss: 4.521\n",
      "(Epoch 741) Total Loss: 12.049, KLD Loss: 1.618, MSE Loss: 10.430\n",
      "(Epoch 742) Total Loss: 10.444, KLD Loss: 1.873, MSE Loss: 8.571\n",
      "(Epoch 743) Total Loss: 12.197, KLD Loss: 1.531, MSE Loss: 10.666\n",
      "(Epoch 744) Total Loss: 5.686, KLD Loss: 1.630, MSE Loss: 4.055\n",
      "(Epoch 745) Total Loss: 14.561, KLD Loss: 1.693, MSE Loss: 12.868\n",
      "(Epoch 746) Total Loss: 5.267, KLD Loss: 1.838, MSE Loss: 3.429\n",
      "(Epoch 747) Total Loss: 4.563, KLD Loss: 1.609, MSE Loss: 2.953\n",
      "(Epoch 748) Total Loss: 6.138, KLD Loss: 1.715, MSE Loss: 4.423\n",
      "(Epoch 749) Total Loss: 3.990, KLD Loss: 1.618, MSE Loss: 2.372\n",
      "(Epoch 750) Total Loss: 4.790, KLD Loss: 1.723, MSE Loss: 3.067\n",
      "(Epoch 751) Total Loss: 6.175, KLD Loss: 1.737, MSE Loss: 4.439\n",
      "(Epoch 752) Total Loss: 8.431, KLD Loss: 1.741, MSE Loss: 6.691\n",
      "(Epoch 753) Total Loss: 5.909, KLD Loss: 1.666, MSE Loss: 4.243\n",
      "(Epoch 754) Total Loss: 4.145, KLD Loss: 1.443, MSE Loss: 2.702\n",
      "(Epoch 755) Total Loss: 5.891, KLD Loss: 1.449, MSE Loss: 4.442\n",
      "(Epoch 756) Total Loss: 8.557, KLD Loss: 1.925, MSE Loss: 6.632\n",
      "(Epoch 757) Total Loss: 4.366, KLD Loss: 1.823, MSE Loss: 2.543\n",
      "(Epoch 758) Total Loss: 3.351, KLD Loss: 1.525, MSE Loss: 1.826\n",
      "(Epoch 759) Total Loss: 12.148, KLD Loss: 1.630, MSE Loss: 10.518\n",
      "(Epoch 760) Total Loss: 11.785, KLD Loss: 1.510, MSE Loss: 10.275\n",
      "(Epoch 761) Total Loss: 3.325, KLD Loss: 1.647, MSE Loss: 1.677\n",
      "(Epoch 762) Total Loss: 7.288, KLD Loss: 1.816, MSE Loss: 5.472\n",
      "(Epoch 763) Total Loss: 8.812, KLD Loss: 1.831, MSE Loss: 6.981\n",
      "(Epoch 764) Total Loss: 5.708, KLD Loss: 1.474, MSE Loss: 4.234\n",
      "(Epoch 765) Total Loss: 4.015, KLD Loss: 1.770, MSE Loss: 2.245\n",
      "(Epoch 766) Total Loss: 3.646, KLD Loss: 1.712, MSE Loss: 1.934\n",
      "(Epoch 767) Total Loss: 9.219, KLD Loss: 1.841, MSE Loss: 7.378\n",
      "(Epoch 768) Total Loss: 5.634, KLD Loss: 1.687, MSE Loss: 3.947\n",
      "(Epoch 769) Total Loss: 11.376, KLD Loss: 1.538, MSE Loss: 9.839\n",
      "(Epoch 770) Total Loss: 6.690, KLD Loss: 1.823, MSE Loss: 4.867\n",
      "(Epoch 771) Total Loss: 6.775, KLD Loss: 1.774, MSE Loss: 5.001\n",
      "(Epoch 772) Total Loss: 4.633, KLD Loss: 1.664, MSE Loss: 2.969\n",
      "(Epoch 773) Total Loss: 16.677, KLD Loss: 1.594, MSE Loss: 15.084\n",
      "(Epoch 774) Total Loss: 9.073, KLD Loss: 1.489, MSE Loss: 7.584\n",
      "(Epoch 775) Total Loss: 7.431, KLD Loss: 1.682, MSE Loss: 5.749\n",
      "(Epoch 776) Total Loss: 5.884, KLD Loss: 1.648, MSE Loss: 4.236\n",
      "(Epoch 777) Total Loss: 3.125, KLD Loss: 1.687, MSE Loss: 1.438\n",
      "(Epoch 778) Total Loss: 10.648, KLD Loss: 1.702, MSE Loss: 8.945\n",
      "(Epoch 779) Total Loss: 5.829, KLD Loss: 1.975, MSE Loss: 3.853\n",
      "(Epoch 780) Total Loss: 7.964, KLD Loss: 1.517, MSE Loss: 6.447\n",
      "(Epoch 781) Total Loss: 4.519, KLD Loss: 1.641, MSE Loss: 2.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 782) Total Loss: 4.164, KLD Loss: 1.656, MSE Loss: 2.508\n",
      "(Epoch 783) Total Loss: 6.549, KLD Loss: 1.602, MSE Loss: 4.946\n",
      "(Epoch 784) Total Loss: 4.894, KLD Loss: 1.418, MSE Loss: 3.476\n",
      "(Epoch 785) Total Loss: 8.073, KLD Loss: 1.971, MSE Loss: 6.103\n",
      "(Epoch 786) Total Loss: 3.879, KLD Loss: 1.710, MSE Loss: 2.169\n",
      "(Epoch 787) Total Loss: 4.774, KLD Loss: 1.356, MSE Loss: 3.417\n",
      "(Epoch 788) Total Loss: 6.671, KLD Loss: 1.678, MSE Loss: 4.993\n",
      "(Epoch 789) Total Loss: 5.196, KLD Loss: 1.687, MSE Loss: 3.508\n",
      "(Epoch 790) Total Loss: 4.774, KLD Loss: 1.751, MSE Loss: 3.023\n",
      "(Epoch 791) Total Loss: 9.436, KLD Loss: 1.524, MSE Loss: 7.913\n",
      "(Epoch 792) Total Loss: 4.964, KLD Loss: 1.637, MSE Loss: 3.327\n",
      "(Epoch 793) Total Loss: 4.723, KLD Loss: 1.684, MSE Loss: 3.039\n",
      "(Epoch 794) Total Loss: 5.795, KLD Loss: 1.862, MSE Loss: 3.933\n",
      "(Epoch 795) Total Loss: 5.362, KLD Loss: 1.710, MSE Loss: 3.652\n",
      "(Epoch 796) Total Loss: 4.389, KLD Loss: 2.055, MSE Loss: 2.335\n",
      "(Epoch 797) Total Loss: 4.482, KLD Loss: 1.587, MSE Loss: 2.896\n",
      "(Epoch 798) Total Loss: 5.755, KLD Loss: 1.809, MSE Loss: 3.946\n",
      "(Epoch 799) Total Loss: 8.288, KLD Loss: 1.775, MSE Loss: 6.513\n",
      "(Epoch 800) Total Loss: 4.963, KLD Loss: 1.516, MSE Loss: 3.447\n",
      "(Epoch 801) Total Loss: 5.647, KLD Loss: 2.044, MSE Loss: 3.603\n",
      "(Epoch 802) Total Loss: 24.298, KLD Loss: 1.493, MSE Loss: 22.806\n",
      "(Epoch 803) Total Loss: 4.818, KLD Loss: 1.661, MSE Loss: 3.156\n",
      "(Epoch 804) Total Loss: 4.400, KLD Loss: 1.535, MSE Loss: 2.865\n",
      "(Epoch 805) Total Loss: 5.406, KLD Loss: 1.900, MSE Loss: 3.505\n",
      "(Epoch 806) Total Loss: 4.103, KLD Loss: 1.867, MSE Loss: 2.236\n",
      "(Epoch 807) Total Loss: 12.421, KLD Loss: 1.432, MSE Loss: 10.989\n",
      "(Epoch 808) Total Loss: 3.935, KLD Loss: 1.657, MSE Loss: 2.278\n",
      "(Epoch 809) Total Loss: 8.820, KLD Loss: 1.819, MSE Loss: 7.000\n",
      "(Epoch 810) Total Loss: 5.683, KLD Loss: 1.759, MSE Loss: 3.924\n",
      "(Epoch 811) Total Loss: 6.020, KLD Loss: 1.754, MSE Loss: 4.267\n",
      "(Epoch 812) Total Loss: 4.873, KLD Loss: 1.492, MSE Loss: 3.381\n",
      "(Epoch 813) Total Loss: 4.626, KLD Loss: 1.866, MSE Loss: 2.759\n",
      "(Epoch 814) Total Loss: 4.311, KLD Loss: 1.573, MSE Loss: 2.738\n",
      "(Epoch 815) Total Loss: 11.491, KLD Loss: 1.659, MSE Loss: 9.832\n",
      "(Epoch 816) Total Loss: 10.362, KLD Loss: 1.684, MSE Loss: 8.679\n",
      "(Epoch 817) Total Loss: 7.271, KLD Loss: 1.668, MSE Loss: 5.603\n",
      "(Epoch 818) Total Loss: 3.817, KLD Loss: 1.395, MSE Loss: 2.422\n",
      "(Epoch 819) Total Loss: 9.808, KLD Loss: 1.793, MSE Loss: 8.015\n",
      "(Epoch 820) Total Loss: 13.479, KLD Loss: 1.791, MSE Loss: 11.688\n",
      "(Epoch 821) Total Loss: 5.324, KLD Loss: 1.586, MSE Loss: 3.738\n",
      "(Epoch 822) Total Loss: 7.718, KLD Loss: 1.787, MSE Loss: 5.930\n",
      "(Epoch 823) Total Loss: 3.246, KLD Loss: 1.651, MSE Loss: 1.595\n",
      "(Epoch 824) Total Loss: 3.780, KLD Loss: 1.451, MSE Loss: 2.329\n",
      "(Epoch 825) Total Loss: 3.742, KLD Loss: 1.378, MSE Loss: 2.364\n",
      "(Epoch 826) Total Loss: 11.564, KLD Loss: 1.953, MSE Loss: 9.611\n",
      "(Epoch 827) Total Loss: 5.572, KLD Loss: 1.817, MSE Loss: 3.755\n",
      "(Epoch 828) Total Loss: 4.722, KLD Loss: 1.536, MSE Loss: 3.186\n",
      "(Epoch 829) Total Loss: 6.853, KLD Loss: 1.613, MSE Loss: 5.240\n",
      "(Epoch 830) Total Loss: 5.273, KLD Loss: 1.784, MSE Loss: 3.490\n",
      "(Epoch 831) Total Loss: 3.283, KLD Loss: 1.714, MSE Loss: 1.569\n",
      "(Epoch 832) Total Loss: 4.063, KLD Loss: 1.609, MSE Loss: 2.454\n",
      "(Epoch 833) Total Loss: 5.496, KLD Loss: 1.895, MSE Loss: 3.601\n",
      "(Epoch 834) Total Loss: 8.836, KLD Loss: 1.755, MSE Loss: 7.082\n",
      "(Epoch 835) Total Loss: 3.307, KLD Loss: 1.830, MSE Loss: 1.477\n",
      "(Epoch 836) Total Loss: 5.083, KLD Loss: 1.536, MSE Loss: 3.547\n",
      "(Epoch 837) Total Loss: 6.027, KLD Loss: 1.433, MSE Loss: 4.594\n",
      "(Epoch 838) Total Loss: 5.244, KLD Loss: 1.830, MSE Loss: 3.414\n",
      "(Epoch 839) Total Loss: 13.935, KLD Loss: 1.788, MSE Loss: 12.147\n",
      "(Epoch 840) Total Loss: 11.675, KLD Loss: 1.634, MSE Loss: 10.041\n",
      "(Epoch 841) Total Loss: 11.770, KLD Loss: 1.586, MSE Loss: 10.184\n",
      "(Epoch 842) Total Loss: 6.235, KLD Loss: 1.750, MSE Loss: 4.485\n",
      "(Epoch 843) Total Loss: 6.562, KLD Loss: 1.603, MSE Loss: 4.959\n",
      "(Epoch 844) Total Loss: 14.672, KLD Loss: 1.744, MSE Loss: 12.928\n",
      "(Epoch 845) Total Loss: 7.256, KLD Loss: 1.814, MSE Loss: 5.443\n",
      "(Epoch 846) Total Loss: 4.631, KLD Loss: 1.849, MSE Loss: 2.783\n",
      "(Epoch 847) Total Loss: 4.248, KLD Loss: 1.703, MSE Loss: 2.546\n",
      "(Epoch 848) Total Loss: 3.050, KLD Loss: 1.737, MSE Loss: 1.313\n",
      "(Epoch 849) Total Loss: 9.193, KLD Loss: 1.703, MSE Loss: 7.491\n",
      "(Epoch 850) Total Loss: 7.934, KLD Loss: 1.804, MSE Loss: 6.129\n",
      "(Epoch 851) Total Loss: 13.823, KLD Loss: 1.929, MSE Loss: 11.894\n",
      "(Epoch 852) Total Loss: 5.412, KLD Loss: 1.839, MSE Loss: 3.572\n",
      "(Epoch 853) Total Loss: 4.009, KLD Loss: 1.693, MSE Loss: 2.316\n",
      "(Epoch 854) Total Loss: 10.853, KLD Loss: 2.011, MSE Loss: 8.842\n",
      "(Epoch 855) Total Loss: 11.577, KLD Loss: 1.631, MSE Loss: 9.946\n",
      "(Epoch 856) Total Loss: 7.772, KLD Loss: 1.385, MSE Loss: 6.388\n",
      "(Epoch 857) Total Loss: 4.796, KLD Loss: 1.373, MSE Loss: 3.423\n",
      "(Epoch 858) Total Loss: 7.038, KLD Loss: 1.849, MSE Loss: 5.189\n",
      "(Epoch 859) Total Loss: 3.943, KLD Loss: 1.404, MSE Loss: 2.539\n",
      "(Epoch 860) Total Loss: 6.647, KLD Loss: 1.463, MSE Loss: 5.184\n",
      "(Epoch 861) Total Loss: 7.527, KLD Loss: 1.405, MSE Loss: 6.122\n",
      "(Epoch 862) Total Loss: 5.195, KLD Loss: 1.766, MSE Loss: 3.430\n",
      "(Epoch 863) Total Loss: 9.167, KLD Loss: 1.641, MSE Loss: 7.525\n",
      "(Epoch 864) Total Loss: 3.261, KLD Loss: 1.655, MSE Loss: 1.606\n",
      "(Epoch 865) Total Loss: 4.946, KLD Loss: 1.689, MSE Loss: 3.257\n",
      "(Epoch 866) Total Loss: 9.692, KLD Loss: 1.712, MSE Loss: 7.980\n",
      "(Epoch 867) Total Loss: 6.253, KLD Loss: 1.817, MSE Loss: 4.436\n",
      "(Epoch 868) Total Loss: 4.390, KLD Loss: 1.413, MSE Loss: 2.977\n",
      "(Epoch 869) Total Loss: 11.278, KLD Loss: 1.858, MSE Loss: 9.421\n",
      "(Epoch 870) Total Loss: 4.701, KLD Loss: 1.696, MSE Loss: 3.005\n",
      "(Epoch 871) Total Loss: 10.290, KLD Loss: 1.793, MSE Loss: 8.497\n",
      "(Epoch 872) Total Loss: 11.481, KLD Loss: 1.673, MSE Loss: 9.808\n",
      "(Epoch 873) Total Loss: 4.360, KLD Loss: 1.686, MSE Loss: 2.674\n",
      "(Epoch 874) Total Loss: 4.243, KLD Loss: 1.519, MSE Loss: 2.724\n",
      "(Epoch 875) Total Loss: 7.550, KLD Loss: 1.815, MSE Loss: 5.736\n",
      "(Epoch 876) Total Loss: 5.675, KLD Loss: 1.873, MSE Loss: 3.802\n",
      "(Epoch 877) Total Loss: 5.993, KLD Loss: 1.785, MSE Loss: 4.208\n",
      "(Epoch 878) Total Loss: 5.703, KLD Loss: 1.768, MSE Loss: 3.935\n",
      "(Epoch 879) Total Loss: 7.939, KLD Loss: 1.495, MSE Loss: 6.444\n",
      "(Epoch 880) Total Loss: 8.960, KLD Loss: 1.422, MSE Loss: 7.539\n",
      "(Epoch 881) Total Loss: 5.944, KLD Loss: 1.645, MSE Loss: 4.299\n",
      "(Epoch 882) Total Loss: 12.813, KLD Loss: 1.842, MSE Loss: 10.971\n",
      "(Epoch 883) Total Loss: 7.035, KLD Loss: 1.618, MSE Loss: 5.417\n",
      "(Epoch 884) Total Loss: 8.220, KLD Loss: 1.728, MSE Loss: 6.492\n",
      "(Epoch 885) Total Loss: 4.374, KLD Loss: 1.580, MSE Loss: 2.794\n",
      "(Epoch 886) Total Loss: 6.274, KLD Loss: 1.622, MSE Loss: 4.651\n",
      "(Epoch 887) Total Loss: 7.733, KLD Loss: 1.652, MSE Loss: 6.081\n",
      "(Epoch 888) Total Loss: 2.947, KLD Loss: 1.573, MSE Loss: 1.374\n",
      "(Epoch 889) Total Loss: 4.118, KLD Loss: 1.501, MSE Loss: 2.617\n",
      "(Epoch 890) Total Loss: 6.785, KLD Loss: 1.558, MSE Loss: 5.227\n",
      "(Epoch 891) Total Loss: 5.389, KLD Loss: 1.546, MSE Loss: 3.843\n",
      "(Epoch 892) Total Loss: 4.671, KLD Loss: 1.613, MSE Loss: 3.059\n",
      "(Epoch 893) Total Loss: 5.887, KLD Loss: 1.699, MSE Loss: 4.188\n",
      "(Epoch 894) Total Loss: 4.451, KLD Loss: 1.643, MSE Loss: 2.808\n",
      "(Epoch 895) Total Loss: 3.358, KLD Loss: 1.661, MSE Loss: 1.697\n",
      "(Epoch 896) Total Loss: 5.406, KLD Loss: 1.712, MSE Loss: 3.694\n",
      "(Epoch 897) Total Loss: 5.331, KLD Loss: 1.867, MSE Loss: 3.464\n",
      "(Epoch 898) Total Loss: 5.460, KLD Loss: 1.607, MSE Loss: 3.853\n",
      "(Epoch 899) Total Loss: 6.786, KLD Loss: 1.986, MSE Loss: 4.800\n",
      "(Epoch 900) Total Loss: 3.317, KLD Loss: 1.602, MSE Loss: 1.715\n",
      "(Epoch 901) Total Loss: 8.704, KLD Loss: 1.805, MSE Loss: 6.899\n",
      "(Epoch 902) Total Loss: 6.596, KLD Loss: 1.882, MSE Loss: 4.714\n",
      "(Epoch 903) Total Loss: 4.215, KLD Loss: 1.641, MSE Loss: 2.573\n",
      "(Epoch 904) Total Loss: 4.791, KLD Loss: 1.602, MSE Loss: 3.189\n",
      "(Epoch 905) Total Loss: 7.074, KLD Loss: 1.728, MSE Loss: 5.346\n",
      "(Epoch 906) Total Loss: 6.090, KLD Loss: 1.666, MSE Loss: 4.425\n",
      "(Epoch 907) Total Loss: 8.048, KLD Loss: 1.639, MSE Loss: 6.409\n",
      "(Epoch 908) Total Loss: 8.215, KLD Loss: 1.572, MSE Loss: 6.642\n",
      "(Epoch 909) Total Loss: 3.451, KLD Loss: 1.414, MSE Loss: 2.037\n",
      "(Epoch 910) Total Loss: 9.124, KLD Loss: 1.657, MSE Loss: 7.467\n",
      "(Epoch 911) Total Loss: 4.754, KLD Loss: 1.758, MSE Loss: 2.997\n",
      "(Epoch 912) Total Loss: 2.648, KLD Loss: 1.601, MSE Loss: 1.047\n",
      "(Epoch 913) Total Loss: 4.742, KLD Loss: 1.773, MSE Loss: 2.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 914) Total Loss: 8.126, KLD Loss: 1.623, MSE Loss: 6.502\n",
      "(Epoch 915) Total Loss: 8.680, KLD Loss: 1.901, MSE Loss: 6.779\n",
      "(Epoch 916) Total Loss: 4.130, KLD Loss: 1.589, MSE Loss: 2.541\n",
      "(Epoch 917) Total Loss: 4.524, KLD Loss: 1.689, MSE Loss: 2.835\n",
      "(Epoch 918) Total Loss: 3.695, KLD Loss: 1.670, MSE Loss: 2.026\n",
      "(Epoch 919) Total Loss: 4.053, KLD Loss: 1.454, MSE Loss: 2.599\n",
      "(Epoch 920) Total Loss: 4.430, KLD Loss: 1.791, MSE Loss: 2.639\n",
      "(Epoch 921) Total Loss: 5.059, KLD Loss: 1.561, MSE Loss: 3.498\n",
      "(Epoch 922) Total Loss: 6.520, KLD Loss: 1.467, MSE Loss: 5.053\n",
      "(Epoch 923) Total Loss: 5.201, KLD Loss: 1.590, MSE Loss: 3.611\n",
      "(Epoch 924) Total Loss: 3.530, KLD Loss: 1.571, MSE Loss: 1.959\n",
      "(Epoch 925) Total Loss: 4.758, KLD Loss: 1.551, MSE Loss: 3.206\n",
      "(Epoch 926) Total Loss: 4.724, KLD Loss: 2.026, MSE Loss: 2.697\n",
      "(Epoch 927) Total Loss: 8.108, KLD Loss: 1.867, MSE Loss: 6.241\n",
      "(Epoch 928) Total Loss: 5.150, KLD Loss: 1.649, MSE Loss: 3.501\n",
      "(Epoch 929) Total Loss: 6.751, KLD Loss: 1.516, MSE Loss: 5.236\n",
      "(Epoch 930) Total Loss: 4.623, KLD Loss: 1.537, MSE Loss: 3.085\n",
      "(Epoch 931) Total Loss: 7.823, KLD Loss: 1.450, MSE Loss: 6.373\n",
      "(Epoch 932) Total Loss: 5.792, KLD Loss: 1.814, MSE Loss: 3.978\n",
      "(Epoch 933) Total Loss: 9.820, KLD Loss: 1.530, MSE Loss: 8.290\n",
      "(Epoch 934) Total Loss: 5.836, KLD Loss: 1.390, MSE Loss: 4.445\n",
      "(Epoch 935) Total Loss: 7.340, KLD Loss: 1.665, MSE Loss: 5.675\n",
      "(Epoch 936) Total Loss: 4.873, KLD Loss: 1.919, MSE Loss: 2.954\n",
      "(Epoch 937) Total Loss: 6.835, KLD Loss: 1.468, MSE Loss: 5.366\n",
      "(Epoch 938) Total Loss: 5.067, KLD Loss: 1.876, MSE Loss: 3.191\n",
      "(Epoch 939) Total Loss: 3.641, KLD Loss: 1.677, MSE Loss: 1.965\n",
      "(Epoch 940) Total Loss: 6.250, KLD Loss: 1.537, MSE Loss: 4.714\n",
      "(Epoch 941) Total Loss: 3.900, KLD Loss: 1.734, MSE Loss: 2.167\n",
      "(Epoch 942) Total Loss: 4.103, KLD Loss: 1.864, MSE Loss: 2.239\n",
      "(Epoch 943) Total Loss: 4.884, KLD Loss: 1.562, MSE Loss: 3.321\n",
      "(Epoch 944) Total Loss: 9.955, KLD Loss: 1.658, MSE Loss: 8.297\n",
      "(Epoch 945) Total Loss: 5.020, KLD Loss: 1.854, MSE Loss: 3.167\n",
      "(Epoch 946) Total Loss: 3.865, KLD Loss: 1.598, MSE Loss: 2.267\n",
      "(Epoch 947) Total Loss: 4.998, KLD Loss: 1.853, MSE Loss: 3.146\n",
      "(Epoch 948) Total Loss: 5.341, KLD Loss: 1.548, MSE Loss: 3.793\n",
      "(Epoch 949) Total Loss: 4.823, KLD Loss: 1.766, MSE Loss: 3.058\n",
      "(Epoch 950) Total Loss: 7.955, KLD Loss: 1.695, MSE Loss: 6.260\n",
      "(Epoch 951) Total Loss: 4.506, KLD Loss: 1.658, MSE Loss: 2.848\n",
      "(Epoch 952) Total Loss: 5.886, KLD Loss: 2.035, MSE Loss: 3.851\n",
      "(Epoch 953) Total Loss: 5.620, KLD Loss: 1.647, MSE Loss: 3.973\n",
      "(Epoch 954) Total Loss: 3.215, KLD Loss: 1.338, MSE Loss: 1.877\n",
      "(Epoch 955) Total Loss: 3.727, KLD Loss: 1.692, MSE Loss: 2.035\n",
      "(Epoch 956) Total Loss: 4.510, KLD Loss: 1.589, MSE Loss: 2.920\n",
      "(Epoch 957) Total Loss: 4.701, KLD Loss: 1.852, MSE Loss: 2.849\n",
      "(Epoch 958) Total Loss: 5.977, KLD Loss: 1.714, MSE Loss: 4.264\n",
      "(Epoch 959) Total Loss: 4.752, KLD Loss: 1.548, MSE Loss: 3.204\n",
      "(Epoch 960) Total Loss: 5.851, KLD Loss: 1.821, MSE Loss: 4.030\n",
      "(Epoch 961) Total Loss: 6.422, KLD Loss: 1.693, MSE Loss: 4.729\n",
      "(Epoch 962) Total Loss: 6.951, KLD Loss: 1.722, MSE Loss: 5.229\n",
      "(Epoch 963) Total Loss: 4.595, KLD Loss: 1.594, MSE Loss: 3.001\n",
      "(Epoch 964) Total Loss: 6.696, KLD Loss: 1.954, MSE Loss: 4.742\n",
      "(Epoch 965) Total Loss: 7.208, KLD Loss: 1.583, MSE Loss: 5.625\n",
      "(Epoch 966) Total Loss: 6.325, KLD Loss: 1.562, MSE Loss: 4.763\n",
      "(Epoch 967) Total Loss: 7.517, KLD Loss: 1.644, MSE Loss: 5.873\n",
      "(Epoch 968) Total Loss: 5.119, KLD Loss: 1.626, MSE Loss: 3.494\n",
      "(Epoch 969) Total Loss: 5.292, KLD Loss: 1.639, MSE Loss: 3.653\n",
      "(Epoch 970) Total Loss: 4.879, KLD Loss: 1.351, MSE Loss: 3.528\n",
      "(Epoch 971) Total Loss: 3.179, KLD Loss: 1.529, MSE Loss: 1.650\n",
      "(Epoch 972) Total Loss: 4.422, KLD Loss: 1.896, MSE Loss: 2.526\n",
      "(Epoch 973) Total Loss: 9.391, KLD Loss: 1.764, MSE Loss: 7.627\n",
      "(Epoch 974) Total Loss: 4.630, KLD Loss: 1.456, MSE Loss: 3.174\n",
      "(Epoch 975) Total Loss: 3.380, KLD Loss: 1.671, MSE Loss: 1.709\n",
      "(Epoch 976) Total Loss: 4.492, KLD Loss: 1.658, MSE Loss: 2.834\n",
      "(Epoch 977) Total Loss: 5.358, KLD Loss: 1.865, MSE Loss: 3.494\n",
      "(Epoch 978) Total Loss: 5.819, KLD Loss: 1.591, MSE Loss: 4.227\n",
      "(Epoch 979) Total Loss: 3.014, KLD Loss: 1.717, MSE Loss: 1.297\n",
      "(Epoch 980) Total Loss: 4.809, KLD Loss: 1.549, MSE Loss: 3.260\n",
      "(Epoch 981) Total Loss: 4.044, KLD Loss: 1.593, MSE Loss: 2.451\n",
      "(Epoch 982) Total Loss: 6.145, KLD Loss: 1.582, MSE Loss: 4.563\n",
      "(Epoch 983) Total Loss: 7.105, KLD Loss: 1.615, MSE Loss: 5.490\n",
      "(Epoch 984) Total Loss: 6.422, KLD Loss: 1.634, MSE Loss: 4.788\n",
      "(Epoch 985) Total Loss: 3.807, KLD Loss: 1.511, MSE Loss: 2.296\n",
      "(Epoch 986) Total Loss: 4.853, KLD Loss: 1.871, MSE Loss: 2.982\n",
      "(Epoch 987) Total Loss: 4.459, KLD Loss: 1.459, MSE Loss: 3.000\n",
      "(Epoch 988) Total Loss: 6.017, KLD Loss: 1.820, MSE Loss: 4.198\n",
      "(Epoch 989) Total Loss: 6.239, KLD Loss: 1.706, MSE Loss: 4.533\n",
      "(Epoch 990) Total Loss: 4.254, KLD Loss: 1.655, MSE Loss: 2.598\n",
      "(Epoch 991) Total Loss: 2.955, KLD Loss: 1.672, MSE Loss: 1.283\n",
      "(Epoch 992) Total Loss: 3.849, KLD Loss: 1.527, MSE Loss: 2.322\n",
      "(Epoch 993) Total Loss: 3.024, KLD Loss: 1.497, MSE Loss: 1.526\n",
      "(Epoch 994) Total Loss: 5.481, KLD Loss: 1.609, MSE Loss: 3.872\n",
      "(Epoch 995) Total Loss: 3.226, KLD Loss: 1.581, MSE Loss: 1.646\n",
      "(Epoch 996) Total Loss: 5.570, KLD Loss: 1.559, MSE Loss: 4.011\n",
      "(Epoch 997) Total Loss: 6.033, KLD Loss: 1.589, MSE Loss: 4.444\n",
      "(Epoch 998) Total Loss: 3.679, KLD Loss: 1.493, MSE Loss: 2.185\n",
      "(Epoch 999) Total Loss: 3.878, KLD Loss: 1.556, MSE Loss: 2.322\n",
      "(Epoch 1000) Total Loss: 6.724, KLD Loss: 1.936, MSE Loss: 4.788\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    X_position, Y_position, X_displacement, Y_displacement = get_random_batches(X_seq_len, Y_seq_len, batch_size)\n",
    "    \n",
    "#     writer.add_scalars('data/X', {\n",
    "#         'x': X.permute(0, 2, 1)[0][0][0],\n",
    "#         'y': Y.permute(0, 2, 1)[0][0][0]\n",
    "#     }, epoch)\n",
    "    \n",
    "    running_kld_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "    encoder1_optimizer.zero_grad()\n",
    "    encoder2_optimizer.zero_grad()\n",
    "    cvae_optimizer.zero_grad()\n",
    "    fcs_optimizer.zero_grad()\n",
    "    decoder1_optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder 1\n",
    "    e1_hidden = encoder1.init_hidden(batch_size)\n",
    "    e1_output, e1_last_hidden = encoder1(X_displacement, e1_hidden)\n",
    "    H_X = e1_last_hidden\n",
    "\n",
    "    # Encoder 2\n",
    "    e2_hidden = encoder2.init_hidden(batch_size)\n",
    "    e2_output, e2_last_hidden = encoder2(Y_displacement, e2_hidden)\n",
    "    H_Y = e2_last_hidden\n",
    "\n",
    "    # CVAE\n",
    "    H_XY = torch.cat([H_X, H_Y], 2)\n",
    "    z = cvae(H_XY)\n",
    "\n",
    "    # FCS\n",
    "    beta_z = fcs(z)\n",
    "    \n",
    "    # Decoder\n",
    "    xz = H_X*beta_z\n",
    "    hxz = xz\n",
    "    for i in range(39):\n",
    "        hxz = torch.cat((hxz, Variable(torch.zeros(1, batch_size, 48)).cuda()), 0)\n",
    "    decoder_hidden = decoder1.init_hidden(batch_size)\n",
    "    output, last_hidden = decoder1(hxz, decoder_hidden)\n",
    "\n",
    "    # Reconstruction\n",
    "    X0 = X_position.permute(2, 0, 1)[-1]\n",
    "    delta_X0 = output[0]    \n",
    "    Y0_hat = X0 + delta_X0\n",
    "    Y_hat = Y0_hat.unsqueeze(0)\n",
    "    \n",
    "    for i in range(1, Y_seq_len):\n",
    "        Yi = Y_hat[i - 1]\n",
    "        delta_Xi = output[i]\n",
    "        Yi_hat = Yi + delta_Xi\n",
    "        Yi_hat = Yi_hat.unsqueeze(0)\n",
    "        Y_hat = torch.cat((Y_hat, Yi_hat), 0)\n",
    "    #Y_hat = Y_hat.permute(1, 2, 0)\n",
    "        \n",
    "    # Minimise loss\n",
    "    # KLD Loss requires random z in N(0,1)\n",
    "    test_z = Variable(torch.normal(torch.zeros(batch_size, 48),\n",
    "                                   torch.ones(batch_size, 48))).cuda()\n",
    "    test_z = test_z.unsqueeze(0)\n",
    "    kld_loss = kld(beta_z, test_z)\n",
    "    \n",
    "    # MSE Loss requires to add displacement at all steps before\n",
    "    Y_true = Y_position.permute(2, 0, 1)\n",
    "    mse_loss = mse(Y_hat, Y_true)\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = kld_loss + mse_loss\n",
    "    loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "    encoder1_optimizer.step()\n",
    "    encoder2_optimizer.step()\n",
    "    cvae_optimizer.step()\n",
    "    fcs_optimizer.step()\n",
    "    decoder1_optimizer.step()\n",
    "    \n",
    "    running_kld_loss += kld_loss.item()\n",
    "    running_mse_loss += mse_loss.item()\n",
    "    running_loss += loss.item()\n",
    "    print('(Epoch %d) Total Loss: %.3f, KLD Loss: %.3f, MSE Loss: %.3f' \n",
    "          % (epoch + 1, running_loss, running_kld_loss, running_mse_loss))\n",
    "    running_kld_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "    \n",
    "    if epoch + 1 == num_epochs:\n",
    "        X_true = X_position\n",
    "        Y_true = Y_true.permute(1, 2, 0)\n",
    "        Y_hat = Y_hat.permute(1, 2, 0)\n",
    "\n",
    "#         print()\n",
    "        \n",
    "#         print('Generated Y:')\n",
    "#         print(Y_hat[0])\n",
    "        \n",
    "#         print_line_sep()\n",
    "        \n",
    "#         print('True Y:')\n",
    "#         print(Y_true[0])\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 15))\n",
    "        fig.tight_layout()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            ax = fig.add_subplot(batch_size/2, batch_size - batch_size/2, i + 1)\n",
    "         \n",
    "            ax.plot(X_true[i][0].cpu().detach().numpy(),\n",
    "                    X_true[i][1].cpu().detach().numpy(),\n",
    "                    linestyle='-', marker='o', color='y', label='Past')\n",
    "        \n",
    "            ax.plot(Y_true[i][0].cpu().detach().numpy(),\n",
    "                    Y_true[i][1].cpu().detach().numpy(),\n",
    "                    linestyle='-', marker='o', color='b', label='True')\n",
    "        \n",
    "            ax.plot(Y_hat[i][0].cpu().detach().numpy(),\n",
    "                    Y_hat[i][1].cpu().detach().numpy(),\n",
    "                    linestyle='-', marker='o', color='r', label='Reconstructed')\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.legend(loc='upper left')\n",
    "#             ax.xlabel('x-coordinate')\n",
    "#             ax.ylabel('y-coordinate')\n",
    "\n",
    "            X_true_x_min = X_true[i][0].cpu().detach().numpy().min()\n",
    "            X_true_x_max = X_true[i][0].cpu().detach().numpy().max()\n",
    "            X_true_y_min = X_true[i][1].cpu().detach().numpy().min()\n",
    "            X_true_y_max = X_true[i][1].cpu().detach().numpy().max()\n",
    "            \n",
    "            Y_true_x_min = Y_true[i][0].cpu().detach().numpy().min()\n",
    "            Y_true_x_max = Y_true[i][0].cpu().detach().numpy().max()\n",
    "            Y_true_y_min = Y_true[i][1].cpu().detach().numpy().min()\n",
    "            Y_true_y_max = Y_true[i][1].cpu().detach().numpy().max()\n",
    "\n",
    "            Y_hat_x_min = Y_hat[i][0].cpu().detach().numpy().min()\n",
    "            Y_hat_x_max = Y_hat[i][0].cpu().detach().numpy().max()\n",
    "            Y_hat_y_min = Y_hat[i][1].cpu().detach().numpy().min()\n",
    "            Y_hat_y_max = Y_hat[i][1].cpu().detach().numpy().min()\n",
    "\n",
    "            xy_x_min = np.sort(np.array([X_true_x_min, Y_true_x_min, Y_hat_x_min]), axis=None)[0]\n",
    "            xy_x_max = np.sort(np.array([X_true_x_max, Y_true_x_max, Y_hat_x_max]), axis=None)[-1]\n",
    "            xy_y_min = np.sort(np.array([X_true_y_min, Y_true_y_min, Y_hat_y_min]), axis=None)[0]\n",
    "            xy_y_max = np.sort(np.array([X_true_y_max, Y_true_y_max, Y_hat_y_max]), axis=None)[-1]\n",
    "            \n",
    "            xy_delta = xy_x_max - xy_x_min if xy_x_max - xy_x_min > xy_y_max - xy_y_min else xy_y_max - xy_y_min\n",
    "            \n",
    "            ax.set_xlim(xy_x_min - 10, xy_x_min + xy_delta + 10)\n",
    "            ax.set_ylim(xy_y_min - 10, xy_y_min + xy_delta + 10)\n",
    "        \n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking & Refinement Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
