{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UROP1100T (Spring 2018)\n",
    "\n",
    "## DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents\n",
    "\n",
    "[CVPR Paper](https://arxiv.org/pdf/1704.04394.pdf)\n",
    "\n",
    "[Supplementary Notes](http://www.robots.ox.ac.uk/~namhoon/doc/DESIRE-supp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "Subtract Y starting from X, not from itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_line_sep():\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data extract:\n",
      "['1', '172', '93.939', '-55.7615', '0', '-3.0066', '5.27149', '4', '10']\n",
      "--------------------------------------\n",
      "Filtered data extract:\n",
      "[1, 172, 93.939, -55.7615]\n",
      "--------------------------------------\n",
      "Data dictionary extract:\n",
      "[1, 93.939, -55.7615]\n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "with open('raw_data/raw_record.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    raw_data = list(csvreader)\n",
    "print('Raw data extract:')\n",
    "print(raw_data[0])\n",
    "print_line_sep()\n",
    "\n",
    "filtered_data = [[int(row[0]), int(row[1]), float(row[2]), float(row[3])] for row in raw_data]\n",
    "print('Filtered data extract:')\n",
    "print(filtered_data[0])\n",
    "print_line_sep()\n",
    "\n",
    "dict_data = {}\n",
    "for row in filtered_data:\n",
    "    time = row[0]\n",
    "    car = row[1]\n",
    "    x = row[2]\n",
    "    y = row[3]\n",
    "    \n",
    "    if car in dict_data:\n",
    "        dict_data[car].append([time, x, y])\n",
    "    else:\n",
    "        dict_data[car] = [[time, x, y]]\n",
    "print('Data dictionary extract:')\n",
    "print(dict_data[172][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batches(x_seq_len, y_seq_len, batch_size):\n",
    "    batches = []\n",
    "    dict_data_keys = list(dict_data.keys())\n",
    "    \n",
    "    while(len(batches) < batch_size): # Fill up batches with batch_size rows of x+y_seq_len columns\n",
    "        car = random.choice(dict_data_keys)\n",
    "        batch = []\n",
    "        is_first = True\n",
    "        \n",
    "        # Loop through each row of that car until x+y_seq_len columns are found\n",
    "        for row in dict_data[car]:\n",
    "            time = row[0]\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            \n",
    "            # If first item of the sequence, just append\n",
    "            if is_first:\n",
    "                batch.append([time, x, y])\n",
    "                is_first = False\n",
    "                \n",
    "            # If not, check if time diff is 1\n",
    "            else:\n",
    "                prev_time = batch[-1][0]\n",
    "                \n",
    "                # If time diff is not 1,\n",
    "                # 1) Clear batch item\n",
    "                # 2) Start from current location as first batch item\n",
    "                if time - prev_time != 1:\n",
    "                    batch = []\n",
    "                    batch.append([time, x, y])\n",
    "                    \n",
    "                # Otherwise, just append to batch item\n",
    "                else:\n",
    "                    batch.append([time, x, y])\n",
    "            \n",
    "            # If batch item columns are enough, break\n",
    "            if len(batch) == x_seq_len + y_seq_len:\n",
    "                batches.append(batch)\n",
    "                break\n",
    "                \n",
    "    # Just keep (x,y)  \n",
    "    X_position_array = [[[item[1], item[2]] for item in batch[:x_seq_len]] for batch in batches]\n",
    "    Y_position_array = [[[item[1], item[2]] for item in batch[x_seq_len:]] for batch in batches]\n",
    "    \n",
    "    X_position_np = np.asarray(X_position_array).transpose(0, 2, 1)\n",
    "    Y_position_np = np.asarray(Y_position_array).transpose(0, 2, 1)\n",
    "    \n",
    "    X_position_tensor = Variable(torch.from_numpy(X_position_np).type(torch.FloatTensor)).cuda()\n",
    "    Y_position_tensor = Variable(torch.from_numpy(Y_position_np).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "    # Convert position to displacement\n",
    "    X_displacement_array = []\n",
    "    Y_displacement_array = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        x_first = X_position_array[i][0][0]\n",
    "        y_first = X_position_array[i][0][1]\n",
    "\n",
    "        X_batch = X_position_array[i]\n",
    "        X_displacement_array.append([[item[0] - x_first, item[1] - y_first] for item in X_batch])\n",
    "\n",
    "        Y_batch = Y_position_array[i]\n",
    "        Y_displacement_array.append([[item[0] - x_first, item[1] - y_first] for item in Y_batch])\n",
    "\n",
    "    X_displacement_np = np.asarray(X_displacement_array).transpose(0, 2, 1)    \n",
    "    Y_displacement_np = np.asarray(Y_displacement_array).transpose(0, 2, 1)\n",
    "    \n",
    "    X_displacement_tensor = Variable(torch.from_numpy(X_displacement_np).type(torch.FloatTensor)).cuda()\n",
    "    Y_displacement_tensor = Variable(torch.from_numpy(Y_displacement_np).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "    return X_position_tensor, Y_position_tensor, X_displacement_tensor, Y_displacement_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X positions:\n",
      "torch.Size([8, 2, 20])\n",
      "tensor([[ 334.9780,  334.9800,  334.9830,  334.9850,  334.9870,  334.9880,\n",
      "          334.9890,  334.9910,  334.9920,  334.9930,  334.9940,  334.9950,\n",
      "          334.9950,  334.9960,  334.9960,  334.9970,  334.9970,  334.9980,\n",
      "          334.9980,  334.9980],\n",
      "        [ -16.6006,  -16.9825,  -17.3609,  -17.7358,  -18.1070,  -18.4746,\n",
      "          -18.8385,  -19.2001,  -19.5669,  -19.9454,  -20.3379,  -20.7343,\n",
      "          -21.1297,  -21.5233,  -21.9147,  -22.3034,  -22.6893,  -23.0718,\n",
      "          -23.4509,  -23.8265]], device='cuda:0')\n",
      "--------------------------------------\n",
      "Y positions:\n",
      "torch.Size([8, 2, 40])\n",
      "tensor([[ 334.9980,  334.9990,  334.9990,  334.9990,  334.9990,  334.9990,\n",
      "          334.9990,  335.0000,  335.0000,  335.0000,  335.0000,  335.0000,\n",
      "          335.0000,  335.0000,  335.0000,  335.0010,  335.0010,  335.0010,\n",
      "          335.0010,  335.0010,  335.0010,  335.0000,  335.0000,  335.0000,\n",
      "          335.0000,  335.0000,  335.0000,  335.0000,  335.0000,  335.0000,\n",
      "          335.0000,  335.0000,  335.0000,  335.0000,  335.0000,  335.0000,\n",
      "          335.0000,  335.0000,  335.0000,  335.0000],\n",
      "        [ -24.1984,  -24.5667,  -24.9312,  -25.2935,  -25.6609,  -26.0400,\n",
      "          -26.4331,  -26.8302,  -27.2262,  -27.6203,  -28.0123,  -28.4016,\n",
      "          -28.7879,  -29.1709,  -29.5505,  -29.9266,  -30.2989,  -30.6676,\n",
      "          -31.0326,  -31.3952,  -31.7630,  -32.1425,  -32.5360,  -32.9334,\n",
      "          -33.3227,  -33.6865,  -33.9355,  -34.1511,  -34.3121,  -34.4174,\n",
      "          -34.4659,  -34.4577,  -34.4109,  -34.3934,  -34.3944,  -34.3964,\n",
      "          -34.3984,  -34.4002,  -34.4017,  -34.4029]], device='cuda:0')\n",
      "--------------------------------------\n",
      "X displacements:\n",
      "torch.Size([8, 2, 20])\n",
      "tensor([[ 0.0000,  0.0020,  0.0050,  0.0070,  0.0090,  0.0100,  0.0110,\n",
      "          0.0130,  0.0140,  0.0150,  0.0160,  0.0170,  0.0170,  0.0180,\n",
      "          0.0180,  0.0190,  0.0190,  0.0200,  0.0200,  0.0200],\n",
      "        [ 0.0000, -0.3819, -0.7603, -1.1352, -1.5064, -1.8740, -2.2379,\n",
      "         -2.5995, -2.9663, -3.3448, -3.7373, -4.1337, -4.5291, -4.9227,\n",
      "         -5.3141, -5.7028, -6.0887, -6.4712, -6.8503, -7.2259]], device='cuda:0')\n",
      "--------------------------------------\n",
      "Y displacements:\n",
      "torch.Size([8, 2, 40])\n",
      "tensor([[  0.0200,   0.0210,   0.0210,   0.0210,   0.0210,   0.0210,\n",
      "           0.0210,   0.0220,   0.0220,   0.0220,   0.0220,   0.0220,\n",
      "           0.0220,   0.0220,   0.0220,   0.0230,   0.0230,   0.0230,\n",
      "           0.0230,   0.0230,   0.0230,   0.0220,   0.0220,   0.0220,\n",
      "           0.0220,   0.0220,   0.0220,   0.0220,   0.0220,   0.0220,\n",
      "           0.0220,   0.0220,   0.0220,   0.0220,   0.0220,   0.0220,\n",
      "           0.0220,   0.0220,   0.0220,   0.0220],\n",
      "        [ -7.5978,  -7.9661,  -8.3306,  -8.6929,  -9.0603,  -9.4394,\n",
      "          -9.8325, -10.2296, -10.6256, -11.0197, -11.4117, -11.8010,\n",
      "         -12.1873, -12.5703, -12.9499, -13.3260, -13.6983, -14.0670,\n",
      "         -14.4320, -14.7946, -15.1624, -15.5419, -15.9354, -16.3328,\n",
      "         -16.7221, -17.0859, -17.3349, -17.5505, -17.7115, -17.8168,\n",
      "         -17.8653, -17.8571, -17.8103, -17.7928, -17.7938, -17.7958,\n",
      "         -17.7978, -17.7996, -17.8011, -17.8023]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X_position_test, Y_position_test, X_displacement_test, Y_displacement_test = get_random_batches(20, 40, 8)\n",
    "\n",
    "print('X positions:')\n",
    "print(X_position_test.size())\n",
    "print(X_position_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('Y positions:')\n",
    "print(Y_position_test.size())\n",
    "print(Y_position_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('X displacements:')\n",
    "print(X_displacement_test.size())\n",
    "print(X_displacement_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('Y displacements:')\n",
    "print(Y_displacement_test.size())\n",
    "print(Y_displacement_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, seq_len, num_layers,\n",
    "                 conv_output_dim, conv_kernel_size,\n",
    "                 gru_hidden_dim):\n",
    "        super(SampleEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_output_dim = conv_output_dim\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        # C = X or Y\n",
    "        # C_i, (input_dim, seq_len) -> tC_i, (conv_output_dim, seq_len)\n",
    "        self.conv = nn.Conv1d(input_dim, conv_output_dim, conv_kernel_size)\n",
    "\n",
    "        # tC_i, (conv_output_dim, seq_len) -> H_C_i, (gru_hidden_dim)\n",
    "        self.gru = nn.GRU(conv_output_dim, gru_hidden_dim, num_layers)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initial hidden vector is hidden_dim-dimensional and padded with 0\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.gru_hidden_dim)).cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        conv_output = self.conv(x)\n",
    "        conv_output = F.relu(conv_output)\n",
    "        \n",
    "        # conv_output has dimensions (batch_size, dim, seq_len)\n",
    "        # GRU accepts input tensor with dimensions (seq_len, batch_size, dim)\n",
    "        # TODO: Pad\n",
    "        conv_output = conv_output.permute(2, 0, 1)\n",
    "        \n",
    "        output, hidden = self.gru(conv_output, hidden)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleCVAE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, mu_dim, sigma_dim):\n",
    "        super(SampleCVAE, self).__init__()\n",
    "        \n",
    "        self.sigma_dim = sigma_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc_mu = nn.Linear(output_dim, mu_dim)\n",
    "        self.fc_sigma = nn.Linear(output_dim, sigma_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        mu = self.fc_mu(output)\n",
    "        sigma = torch.div(torch.exp(self.fc_sigma(output)), 2)\n",
    "        # Reparam trick\n",
    "        epsilon = Variable(torch.normal(torch.zeros(batch_size, self.sigma_dim),\n",
    "                           torch.ones(batch_size, self.sigma_dim))).cuda()\n",
    "\n",
    "        return mu + sigma*epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleFCS(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SampleFCS, self).__init__()\n",
    "        \n",
    "        self.fcs = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fcs(x)\n",
    "        output = F.softmax(output, dim=2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, num_layers, gru_hidden_dim, gru_output_dim, output_dim):\n",
    "        super(SampleDecoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, gru_output_dim, num_layers)\n",
    "        self.linear = nn.Linear(gru_output_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.gru_hidden_dim)).cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "X_seq_len = 20\n",
    "Y_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "# input_dim = 2        | input is a sequence of (x,y) coordinates (i.e. 2-dimensional)\n",
    "# seq_len = 20         | time sequence length of 20\n",
    "# conv_output_dim = 16 | 1D convolution with 16 output channels\n",
    "# conv_kernel_size = 3 | 1D convolution with kernel of width 3\n",
    "# gru_hidden_dim = 48  | 48-dimensional hidden vector\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 2, 20)\n",
    "encoder1 = SampleEncoder(input_dim=2, seq_len=X_seq_len, num_layers=1,\n",
    "                         conv_output_dim=16, conv_kernel_size=3,\n",
    "                         gru_hidden_dim=48).cuda()\n",
    "\n",
    "# input_dim = 2        | input is a sequence of (x,y) coordinates (i.e. 2-dimensional)\n",
    "# seq_len = 40         | time sequence length of 40\n",
    "# conv_output_dim = 16 | 1D convolution with 16 output channels\n",
    "# conv_kernel_size = 1 | 1D convolution with kernel of width 1\n",
    "# gru_hidden_dim = 48  | 48-dimensional hidden vector\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 2, 40)\n",
    "encoder2 = SampleEncoder(input_dim=2, seq_len=Y_seq_len, num_layers=1,\n",
    "                         conv_output_dim=16, conv_kernel_size=1,\n",
    "                         gru_hidden_dim=48).cuda()\n",
    "\n",
    "# input_dim = 96       | Concatenate encoder1's and encoder2's outputs (48-dim each) into 1 output (96-dim)\n",
    "# output_dim = 48      | Transform concatenated 96-dim vector into 48-dim vector\n",
    "# mu_dim = 48          | mu is 48-dim\n",
    "# sigma_dim = 48       | sigma is 48-dim\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 48)\n",
    "cvae = SampleCVAE(input_dim=96, output_dim=48, mu_dim=48, sigma_dim=48).cuda()\n",
    "\n",
    "# input_dim = 48       |\n",
    "# output_dim = 48      |\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 48)\n",
    "fcs = SampleFCS(input_dim=48, output_dim=48).cuda()\n",
    "\n",
    "# input_dim = 48       |\n",
    "# seq_len = 40         | time sequence length of 40\n",
    "# gru_hidden_dim = 48  |\n",
    "# gru_output_dim = 48  |\n",
    "# output_dim = 2       |\n",
    "#\n",
    "# Input: Tensor of size (40, batch_size, 48)\n",
    "decoder = SampleDecoder(input_dim=48, seq_len=40, num_layers=1,\n",
    "                        gru_hidden_dim=48, gru_output_dim=48,\n",
    "                        output_dim=2).cuda()\n",
    "\n",
    "# Loss\n",
    "kld = nn.KLDivLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': encoder1.parameters()},\n",
    "    {'params': encoder2.parameters()},\n",
    "    {'params': cvae.parameters()},\n",
    "    {'params': fcs.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "], lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) Total Loss: 21.745, KLD Loss: 1.594, MSE Loss: 20.151\n",
      "(Epoch 2) Total Loss: 41.817, KLD Loss: 1.596, MSE Loss: 40.221\n",
      "(Epoch 3) Total Loss: 13.515, KLD Loss: 1.244, MSE Loss: 12.272\n",
      "(Epoch 4) Total Loss: 25.543, KLD Loss: 1.554, MSE Loss: 23.989\n",
      "(Epoch 5) Total Loss: 19.567, KLD Loss: 1.521, MSE Loss: 18.046\n",
      "(Epoch 6) Total Loss: 34.138, KLD Loss: 1.813, MSE Loss: 32.325\n",
      "(Epoch 7) Total Loss: 11.286, KLD Loss: 1.550, MSE Loss: 9.736\n",
      "(Epoch 8) Total Loss: 37.321, KLD Loss: 1.443, MSE Loss: 35.878\n",
      "(Epoch 9) Total Loss: 31.999, KLD Loss: 1.617, MSE Loss: 30.382\n",
      "(Epoch 10) Total Loss: 51.761, KLD Loss: 1.561, MSE Loss: 50.200\n",
      "(Epoch 11) Total Loss: 18.997, KLD Loss: 1.282, MSE Loss: 17.714\n",
      "(Epoch 12) Total Loss: 15.166, KLD Loss: 1.452, MSE Loss: 13.714\n",
      "(Epoch 13) Total Loss: 32.533, KLD Loss: 1.643, MSE Loss: 30.890\n",
      "(Epoch 14) Total Loss: 23.137, KLD Loss: 1.645, MSE Loss: 21.492\n",
      "(Epoch 15) Total Loss: 58.935, KLD Loss: 1.443, MSE Loss: 57.492\n",
      "(Epoch 16) Total Loss: 9.703, KLD Loss: 1.607, MSE Loss: 8.095\n",
      "(Epoch 17) Total Loss: 44.159, KLD Loss: 1.454, MSE Loss: 42.704\n",
      "(Epoch 18) Total Loss: 17.252, KLD Loss: 1.549, MSE Loss: 15.703\n",
      "(Epoch 19) Total Loss: 14.516, KLD Loss: 1.634, MSE Loss: 12.882\n",
      "(Epoch 20) Total Loss: 20.571, KLD Loss: 1.820, MSE Loss: 18.751\n",
      "(Epoch 21) Total Loss: 18.681, KLD Loss: 1.722, MSE Loss: 16.959\n",
      "(Epoch 22) Total Loss: 12.358, KLD Loss: 1.716, MSE Loss: 10.642\n",
      "(Epoch 23) Total Loss: 26.156, KLD Loss: 1.523, MSE Loss: 24.633\n",
      "(Epoch 24) Total Loss: 16.664, KLD Loss: 1.466, MSE Loss: 15.198\n",
      "(Epoch 25) Total Loss: 18.128, KLD Loss: 1.638, MSE Loss: 16.490\n",
      "(Epoch 26) Total Loss: 24.617, KLD Loss: 1.568, MSE Loss: 23.049\n",
      "(Epoch 27) Total Loss: 30.203, KLD Loss: 1.420, MSE Loss: 28.782\n",
      "(Epoch 28) Total Loss: 19.767, KLD Loss: 1.419, MSE Loss: 18.348\n",
      "(Epoch 29) Total Loss: 29.582, KLD Loss: 1.645, MSE Loss: 27.936\n",
      "(Epoch 30) Total Loss: 31.244, KLD Loss: 1.363, MSE Loss: 29.882\n",
      "(Epoch 31) Total Loss: 27.731, KLD Loss: 1.797, MSE Loss: 25.934\n",
      "(Epoch 32) Total Loss: 16.901, KLD Loss: 1.670, MSE Loss: 15.231\n",
      "(Epoch 33) Total Loss: 17.032, KLD Loss: 1.493, MSE Loss: 15.539\n",
      "(Epoch 34) Total Loss: 27.742, KLD Loss: 1.590, MSE Loss: 26.152\n",
      "(Epoch 35) Total Loss: 34.159, KLD Loss: 1.574, MSE Loss: 32.585\n",
      "(Epoch 36) Total Loss: 21.518, KLD Loss: 1.847, MSE Loss: 19.671\n",
      "(Epoch 37) Total Loss: 28.212, KLD Loss: 1.809, MSE Loss: 26.403\n",
      "(Epoch 38) Total Loss: 19.867, KLD Loss: 1.773, MSE Loss: 18.094\n",
      "(Epoch 39) Total Loss: 14.525, KLD Loss: 1.618, MSE Loss: 12.907\n",
      "(Epoch 40) Total Loss: 35.024, KLD Loss: 1.700, MSE Loss: 33.324\n",
      "(Epoch 41) Total Loss: 44.276, KLD Loss: 1.622, MSE Loss: 42.654\n",
      "(Epoch 42) Total Loss: 18.751, KLD Loss: 1.595, MSE Loss: 17.156\n",
      "(Epoch 43) Total Loss: 30.183, KLD Loss: 1.610, MSE Loss: 28.572\n",
      "(Epoch 44) Total Loss: 23.429, KLD Loss: 1.431, MSE Loss: 21.998\n",
      "(Epoch 45) Total Loss: 32.454, KLD Loss: 1.640, MSE Loss: 30.815\n",
      "(Epoch 46) Total Loss: 39.183, KLD Loss: 1.656, MSE Loss: 37.527\n",
      "(Epoch 47) Total Loss: 13.363, KLD Loss: 1.274, MSE Loss: 12.089\n",
      "(Epoch 48) Total Loss: 13.474, KLD Loss: 1.730, MSE Loss: 11.744\n",
      "(Epoch 49) Total Loss: 20.937, KLD Loss: 1.569, MSE Loss: 19.368\n",
      "(Epoch 50) Total Loss: 21.875, KLD Loss: 1.618, MSE Loss: 20.257\n",
      "(Epoch 51) Total Loss: 22.967, KLD Loss: 1.484, MSE Loss: 21.483\n",
      "(Epoch 52) Total Loss: 27.924, KLD Loss: 1.506, MSE Loss: 26.418\n",
      "(Epoch 53) Total Loss: 30.354, KLD Loss: 1.510, MSE Loss: 28.844\n",
      "(Epoch 54) Total Loss: 30.358, KLD Loss: 1.669, MSE Loss: 28.689\n",
      "(Epoch 55) Total Loss: 56.471, KLD Loss: 1.613, MSE Loss: 54.858\n",
      "(Epoch 56) Total Loss: 26.944, KLD Loss: 1.397, MSE Loss: 25.547\n",
      "(Epoch 57) Total Loss: 18.887, KLD Loss: 1.559, MSE Loss: 17.329\n",
      "(Epoch 58) Total Loss: 27.490, KLD Loss: 1.516, MSE Loss: 25.974\n",
      "(Epoch 59) Total Loss: 26.710, KLD Loss: 1.683, MSE Loss: 25.027\n",
      "(Epoch 60) Total Loss: 22.431, KLD Loss: 1.503, MSE Loss: 20.927\n",
      "(Epoch 61) Total Loss: 25.189, KLD Loss: 1.553, MSE Loss: 23.636\n",
      "(Epoch 62) Total Loss: 22.608, KLD Loss: 1.741, MSE Loss: 20.867\n",
      "(Epoch 63) Total Loss: 22.365, KLD Loss: 1.714, MSE Loss: 20.651\n",
      "(Epoch 64) Total Loss: 24.177, KLD Loss: 1.551, MSE Loss: 22.626\n",
      "(Epoch 65) Total Loss: 19.056, KLD Loss: 1.618, MSE Loss: 17.438\n",
      "(Epoch 66) Total Loss: 20.520, KLD Loss: 1.535, MSE Loss: 18.986\n",
      "(Epoch 67) Total Loss: 28.433, KLD Loss: 1.402, MSE Loss: 27.031\n",
      "(Epoch 68) Total Loss: 33.037, KLD Loss: 1.520, MSE Loss: 31.517\n",
      "(Epoch 69) Total Loss: 28.885, KLD Loss: 1.656, MSE Loss: 27.229\n",
      "(Epoch 70) Total Loss: 21.016, KLD Loss: 1.585, MSE Loss: 19.432\n",
      "(Epoch 71) Total Loss: 21.898, KLD Loss: 1.640, MSE Loss: 20.259\n",
      "(Epoch 72) Total Loss: 20.080, KLD Loss: 1.741, MSE Loss: 18.340\n",
      "(Epoch 73) Total Loss: 33.804, KLD Loss: 1.578, MSE Loss: 32.226\n",
      "(Epoch 74) Total Loss: 8.528, KLD Loss: 1.443, MSE Loss: 7.084\n",
      "(Epoch 75) Total Loss: 13.174, KLD Loss: 1.599, MSE Loss: 11.575\n",
      "(Epoch 76) Total Loss: 24.846, KLD Loss: 1.651, MSE Loss: 23.195\n",
      "(Epoch 77) Total Loss: 26.272, KLD Loss: 1.577, MSE Loss: 24.694\n",
      "(Epoch 78) Total Loss: 65.202, KLD Loss: 1.435, MSE Loss: 63.767\n",
      "(Epoch 79) Total Loss: 26.836, KLD Loss: 1.769, MSE Loss: 25.067\n",
      "(Epoch 80) Total Loss: 16.039, KLD Loss: 1.641, MSE Loss: 14.398\n",
      "(Epoch 81) Total Loss: 25.009, KLD Loss: 1.516, MSE Loss: 23.492\n",
      "(Epoch 82) Total Loss: 17.966, KLD Loss: 1.590, MSE Loss: 16.376\n",
      "(Epoch 83) Total Loss: 27.630, KLD Loss: 1.768, MSE Loss: 25.862\n",
      "(Epoch 84) Total Loss: 21.968, KLD Loss: 1.486, MSE Loss: 20.482\n",
      "(Epoch 85) Total Loss: 17.689, KLD Loss: 1.551, MSE Loss: 16.138\n",
      "(Epoch 86) Total Loss: 47.513, KLD Loss: 1.427, MSE Loss: 46.086\n",
      "(Epoch 87) Total Loss: 37.974, KLD Loss: 1.274, MSE Loss: 36.701\n",
      "(Epoch 88) Total Loss: 25.128, KLD Loss: 1.601, MSE Loss: 23.527\n",
      "(Epoch 89) Total Loss: 24.608, KLD Loss: 1.406, MSE Loss: 23.202\n",
      "(Epoch 90) Total Loss: 20.375, KLD Loss: 1.475, MSE Loss: 18.900\n",
      "(Epoch 91) Total Loss: 17.234, KLD Loss: 1.560, MSE Loss: 15.674\n",
      "(Epoch 92) Total Loss: 15.199, KLD Loss: 1.660, MSE Loss: 13.538\n",
      "(Epoch 93) Total Loss: 24.905, KLD Loss: 1.452, MSE Loss: 23.454\n",
      "(Epoch 94) Total Loss: 13.607, KLD Loss: 1.573, MSE Loss: 12.034\n",
      "(Epoch 95) Total Loss: 17.069, KLD Loss: 1.556, MSE Loss: 15.513\n",
      "(Epoch 96) Total Loss: 20.010, KLD Loss: 1.379, MSE Loss: 18.631\n",
      "(Epoch 97) Total Loss: 20.294, KLD Loss: 1.470, MSE Loss: 18.824\n",
      "(Epoch 98) Total Loss: 21.781, KLD Loss: 1.371, MSE Loss: 20.410\n",
      "(Epoch 99) Total Loss: 22.191, KLD Loss: 1.628, MSE Loss: 20.563\n",
      "(Epoch 100) Total Loss: 24.732, KLD Loss: 1.611, MSE Loss: 23.121\n",
      "\n",
      "Generated Y:\n",
      "tensor([[ 173.5624,  173.5157,  173.4649,  173.4133,  173.3620,  173.3113,\n",
      "          173.2610,  173.2111,  173.1615,  173.1121,  173.0628,  173.0136,\n",
      "          172.9644,  172.9153,  172.8662,  172.8171,  172.7680,  172.7189,\n",
      "          172.6698,  172.6207,  172.5716,  172.5225,  172.4734,  172.4243,\n",
      "          172.3753,  172.3262,  172.2771,  172.2280,  172.1789,  172.1298,\n",
      "          172.0807,  172.0316,  171.9826,  171.9335,  171.8844,  171.8353,\n",
      "          171.7862,  171.7371,  171.6880,  171.6389],\n",
      "        [   1.8531,    1.8544,    1.8539,    1.8495,    1.8416,    1.8311,\n",
      "            1.8188,    1.8054,    1.7914,    1.7771,    1.7625,    1.7479,\n",
      "            1.7332,    1.7186,    1.7039,    1.6892,    1.6746,    1.6599,\n",
      "            1.6453,    1.6306,    1.6160,    1.6013,    1.5867,    1.5720,\n",
      "            1.5574,    1.5428,    1.5281,    1.5135,    1.4988,    1.4842,\n",
      "            1.4695,    1.4549,    1.4403,    1.4256,    1.4110,    1.3963,\n",
      "            1.3817,    1.3671,    1.3524,    1.3378]], device='cuda:0')\n",
      "torch.Size([8, 2, 40])\n",
      "--------------------------------------\n",
      "True Y:\n",
      "tensor([[ 173.5970,  173.5970,  173.5970,  173.5970,  173.5970,  173.5970,\n",
      "          173.5970,  173.5970,  173.5970,  173.5970,  173.5970,  173.5970,\n",
      "          173.5970,  173.5970,  173.5970,  173.5970,  173.5970,  173.5970,\n",
      "          173.5970,  173.5970,  173.5970,  173.5970,  173.5970,  173.5970,\n",
      "          173.5970,  173.5970,  173.5970,  173.5970,  173.5970,  173.5970,\n",
      "          173.5970,  173.5970,  173.5970,  173.5970,  173.5970,  173.5970,\n",
      "          173.5970,  173.5970,  173.5970,  173.5970],\n",
      "        [   1.8588,    1.8588,    1.8588,    1.8588,    1.8588,    1.8588,\n",
      "            1.8588,    1.8588,    1.8588,    1.8588,    1.8588,    1.8588,\n",
      "            1.8588,    1.8588,    1.8588,    1.8588,    1.8588,    1.8588,\n",
      "            1.8588,    1.8588,    1.8588,    1.8588,    1.8588,    1.8588,\n",
      "            1.8588,    1.8588,    1.8588,    1.8588,    1.8588,    1.8588,\n",
      "            1.8588,    1.8588,    1.8588,    1.8588,    1.8588,    1.8588,\n",
      "            1.8588,    1.8588,    1.8588,    1.8588]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    X_position, Y_position, X_displacement, Y_displacement = get_random_batches(X_seq_len, Y_seq_len, batch_size)\n",
    "    \n",
    "#     writer.add_scalars('data/X', {\n",
    "#         'x': X.permute(0, 2, 1)[0][0][0],\n",
    "#         'y': Y.permute(0, 2, 1)[0][0][0]\n",
    "#     }, epoch)\n",
    "    \n",
    "    running_kld_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder 1\n",
    "    e1_hidden = encoder1.init_hidden(batch_size)\n",
    "    e1_output, e1_last_hidden = encoder1(X_displacement, e1_hidden)\n",
    "    H_X = e1_last_hidden\n",
    "\n",
    "    # Encoder 2\n",
    "    e2_hidden = encoder2.init_hidden(batch_size)\n",
    "    e2_output, e2_last_hidden = encoder2(Y_displacement, e2_hidden)\n",
    "    H_Y = e2_last_hidden\n",
    "\n",
    "    # CVAE\n",
    "    H_XY = torch.cat([H_X, H_Y], 2)\n",
    "    z = cvae(H_XY)\n",
    "\n",
    "    # FCS\n",
    "    beta_z = fcs(z)\n",
    "    \n",
    "    # Decoder\n",
    "    xz = H_X*beta_z\n",
    "    hxz = xz\n",
    "    for i in range(39):\n",
    "        hxz = torch.cat((hxz, Variable(torch.zeros(1, batch_size, 48)).cuda()), 0)\n",
    "    decoder_hidden = decoder.init_hidden(batch_size)\n",
    "    output, last_hidden = decoder(hxz, decoder_hidden)\n",
    "\n",
    "    # Reconstruction\n",
    "    X0 = X_position.permute(2, 0, 1)[-1]\n",
    "    delta_X0 = output[0]    \n",
    "    Y0_hat = X0 + delta_X0\n",
    "    Y_hat = Y0_hat.unsqueeze(0)\n",
    "    \n",
    "    for i in range(1, Y_seq_len):\n",
    "        Yi = Y_hat[i - 1]\n",
    "        delta_Xi = output[i]\n",
    "        Yi_hat = Yi + delta_Xi\n",
    "        Yi_hat = Yi_hat.unsqueeze(0)\n",
    "        Y_hat = torch.cat((Y_hat, Yi_hat), 0)\n",
    "    Y_hat = Y_hat.permute(1, 2, 0)\n",
    "        \n",
    "    # Minimise loss\n",
    "    # KLD Loss requires random z in N(0,1)\n",
    "    test_z = Variable(torch.normal(torch.zeros(batch_size, 48),\n",
    "                                   torch.ones(batch_size, 48))).cuda()\n",
    "    test_z = test_z.unsqueeze(0)\n",
    "    kld_loss = kld(torch.log(beta_z), test_z)\n",
    "    \n",
    "    # MSE Loss requires to add displacement at all steps before\n",
    "    Y_true = Y_position\n",
    "    mse_loss = mse(Y_hat, Y_true)\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = kld_loss + mse_loss\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    running_kld_loss += kld_loss.item()\n",
    "    running_mse_loss += mse_loss.item()\n",
    "    running_loss += loss.item()\n",
    "    print('(Epoch %d) Total Loss: %.3f, KLD Loss: %.3f, MSE Loss: %.3f' \n",
    "          % (epoch + 1, running_loss, running_kld_loss, running_mse_loss))\n",
    "    running_kld_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "    \n",
    "    if epoch + 1 == num_epochs:\n",
    "        print()\n",
    "        \n",
    "        print('Generated Y:')\n",
    "        print(Y_hat[0])\n",
    "        print(Y_hat.size())\n",
    "        \n",
    "        print_line_sep()\n",
    "        \n",
    "        print('True Y:')\n",
    "        print(Y_true[0])\n",
    "    \n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking & Refinement Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
