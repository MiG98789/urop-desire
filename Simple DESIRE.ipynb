{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UROP1100T (Spring 2018)\n",
    "\n",
    "## DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents\n",
    "\n",
    "[CVPR Paper](https://arxiv.org/pdf/1704.04394.pdf)\n",
    "\n",
    "[Supplementary Notes](http://www.robots.ox.ac.uk/~namhoon/doc/DESIRE-supp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "Subtract Y starting from X, not from itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_line_sep():\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data extract:\n",
      "['1', '172', '93.939', '-55.7615', '0', '-3.0066', '5.27149', '4', '10']\n",
      "--------------------------------------\n",
      "Filtered data extract:\n",
      "[1, 172, 93.939, -55.7615]\n",
      "--------------------------------------\n",
      "Data dictionary extract:\n",
      "[1, 93.939, -55.7615]\n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "with open('raw_data/raw_record.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    raw_data = list(csvreader)\n",
    "print('Raw data extract:')\n",
    "print(raw_data[0])\n",
    "print_line_sep()\n",
    "\n",
    "filtered_data = [[int(row[0]), int(row[1]), float(row[2]), float(row[3])] for row in raw_data]\n",
    "print('Filtered data extract:')\n",
    "print(filtered_data[0])\n",
    "print_line_sep()\n",
    "\n",
    "dict_data = {}\n",
    "for row in filtered_data:\n",
    "    time = row[0]\n",
    "    car = row[1]\n",
    "    x = row[2]\n",
    "    y = row[3]\n",
    "    \n",
    "    if car in dict_data:\n",
    "        dict_data[car].append([time, x, y])\n",
    "    else:\n",
    "        dict_data[car] = [[time, x, y]]\n",
    "print('Data dictionary extract:')\n",
    "print(dict_data[172][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batches(x_seq_len, y_seq_len, batch_size):\n",
    "    batches = []\n",
    "    dict_data_keys = list(dict_data.keys())\n",
    "    \n",
    "    while(len(batches) < batch_size): # Fill up batches with batch_size rows of x+y_seq_len columns\n",
    "        car = random.choice(dict_data_keys)\n",
    "        batch = []\n",
    "        is_first = True\n",
    "        \n",
    "        # Loop through each row of that car until x+y_seq_len columns are found\n",
    "        for row in dict_data[car]:\n",
    "            time = row[0]\n",
    "            x = row[1]\n",
    "            y = row[2]\n",
    "            \n",
    "            # If first item of the sequence, just append\n",
    "            if is_first:\n",
    "                batch.append([time, x, y])\n",
    "                is_first = False\n",
    "                \n",
    "            # If not, check if time diff is 1\n",
    "            else:\n",
    "                prev_time = batch[-1][0]\n",
    "                \n",
    "                # If time diff is not 1,\n",
    "                # 1) Clear batch item\n",
    "                # 2) Start from current location as first batch item\n",
    "                if time - prev_time != 1:\n",
    "                    batch = []\n",
    "                    batch.append([time, x, y])\n",
    "                    \n",
    "                # Otherwise, just append to batch item\n",
    "                else:\n",
    "                    batch.append([time, x, y])\n",
    "            \n",
    "            # If batch item columns are enough, break\n",
    "            if len(batch) == x_seq_len + y_seq_len:\n",
    "                batches.append(batch)\n",
    "                break\n",
    "                \n",
    "    # Just keep (x,y)  \n",
    "    X_position_array = [[[item[1], item[2]] for item in batch[:x_seq_len]] for batch in batches]\n",
    "    Y_position_array = [[[item[1], item[2]] for item in batch[x_seq_len:]] for batch in batches]\n",
    "    \n",
    "    X_position_np = np.asarray(X_position_array).transpose(0, 2, 1)\n",
    "    Y_position_np = np.asarray(Y_position_array).transpose(0, 2, 1)\n",
    "    \n",
    "    X_position_tensor = Variable(torch.from_numpy(X_position_np).type(torch.FloatTensor)).cuda()\n",
    "    Y_position_tensor = Variable(torch.from_numpy(Y_position_np).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "    # Convert position to displacement\n",
    "    X_displacement_array = []\n",
    "    Y_displacement_array = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        x_first = X_position_array[i][0][0]\n",
    "        y_first = X_position_array[i][0][1]\n",
    "\n",
    "        X_batch = X_position_array[i]\n",
    "        X_displacement_array.append([[item[0] - x_first, item[1] - y_first] for item in X_batch])\n",
    "\n",
    "        Y_batch = Y_position_array[i]\n",
    "        Y_displacement_array.append([[item[0] - x_first, item[1] - y_first] for item in Y_batch])\n",
    "\n",
    "    X_displacement_np = np.asarray(X_displacement_array).transpose(0, 2, 1)    \n",
    "    Y_displacement_np = np.asarray(Y_displacement_array).transpose(0, 2, 1)\n",
    "    \n",
    "    X_displacement_tensor = Variable(torch.from_numpy(X_displacement_np).type(torch.FloatTensor)).cuda()\n",
    "    Y_displacement_tensor = Variable(torch.from_numpy(Y_displacement_np).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "    return X_position_tensor, Y_position_tensor, X_displacement_tensor, Y_displacement_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X positions:\n",
      "torch.Size([8, 2, 20])\n",
      "tensor([[ 153.8230,  153.8220,  153.8210,  153.8200,  153.8190,  153.8190,\n",
      "          153.8180,  153.8180,  153.8180,  153.8180,  153.8180,  153.8170,\n",
      "          153.8170,  153.8170,  153.8170,  153.8170,  153.8170,  153.8170,\n",
      "          153.8170,  153.8170],\n",
      "        [ -16.2877,  -16.6703,  -17.0653,  -17.4653,  -17.8652,  -18.2639,\n",
      "          -18.6616,  -19.0583,  -19.4541,  -19.8488,  -20.2423,  -20.6345,\n",
      "          -21.0253,  -21.4146,  -21.8024,  -22.1887,  -22.5733,  -22.9562,\n",
      "          -23.3376,  -23.7172]], device='cuda:0')\n",
      "--------------------------------------\n",
      "Y positions:\n",
      "torch.Size([8, 2, 40])\n",
      "tensor([[ 153.8170,  153.8170,  153.8170,  153.8170,  153.8170,  153.8170,\n",
      "          153.8170,  153.8170,  153.8170,  153.8170,  153.8170,  153.8170,\n",
      "          153.8170,  153.8170,  153.8170,  153.8170,  153.8170,  153.8180,\n",
      "          153.8180,  153.8180,  153.8180,  153.8180,  153.8180,  153.8180,\n",
      "          153.8180,  153.8180,  153.8180,  153.8180,  153.8180,  153.8180,\n",
      "          153.8180,  153.8170,  153.8180,  153.8180,  153.8190,  153.8190,\n",
      "          153.8200,  153.8200,  153.8200,  153.8200],\n",
      "        [ -24.0952,  -24.4715,  -24.8461,  -25.2191,  -25.5905,  -25.9602,\n",
      "          -26.3283,  -26.6959,  -27.0686,  -27.4520,  -27.8479,  -28.2487,\n",
      "          -28.6495,  -29.0490,  -29.4475,  -29.8451,  -30.2416,  -30.6371,\n",
      "          -31.0313,  -31.4242,  -31.8158,  -32.2058,  -32.5943,  -32.9813,\n",
      "          -33.3666,  -33.7502,  -34.1262,  -34.4620,  -34.7009,  -34.8902,\n",
      "          -35.0343,  -35.1392,  -35.2117,  -35.2418,  -35.2309,  -35.1881,\n",
      "          -35.1829,  -35.1847,  -35.1884,  -35.1926]], device='cuda:0')\n",
      "--------------------------------------\n",
      "X displacements:\n",
      "torch.Size([8, 2, 20])\n",
      "tensor([[ 0.0000, -0.0010, -0.0020, -0.0030, -0.0040, -0.0040, -0.0050,\n",
      "         -0.0050, -0.0050, -0.0050, -0.0050, -0.0060, -0.0060, -0.0060,\n",
      "         -0.0060, -0.0060, -0.0060, -0.0060, -0.0060, -0.0060],\n",
      "        [ 0.0000, -0.3826, -0.7776, -1.1776, -1.5775, -1.9762, -2.3739,\n",
      "         -2.7706, -3.1664, -3.5611, -3.9546, -4.3468, -4.7376, -5.1269,\n",
      "         -5.5147, -5.9010, -6.2856, -6.6685, -7.0499, -7.4295]], device='cuda:0')\n",
      "--------------------------------------\n",
      "Y displacements:\n",
      "torch.Size([8, 2, 40])\n",
      "tensor([[ -0.0060,  -0.0060,  -0.0060,  -0.0060,  -0.0060,  -0.0060,\n",
      "          -0.0060,  -0.0060,  -0.0060,  -0.0060,  -0.0060,  -0.0060,\n",
      "          -0.0060,  -0.0060,  -0.0060,  -0.0060,  -0.0060,  -0.0050,\n",
      "          -0.0050,  -0.0050,  -0.0050,  -0.0050,  -0.0050,  -0.0050,\n",
      "          -0.0050,  -0.0050,  -0.0050,  -0.0050,  -0.0050,  -0.0050,\n",
      "          -0.0050,  -0.0060,  -0.0050,  -0.0050,  -0.0040,  -0.0040,\n",
      "          -0.0030,  -0.0030,  -0.0030,  -0.0030],\n",
      "        [ -7.8075,  -8.1838,  -8.5584,  -8.9314,  -9.3028,  -9.6725,\n",
      "         -10.0406, -10.4082, -10.7809, -11.1643, -11.5602, -11.9610,\n",
      "         -12.3618, -12.7613, -13.1598, -13.5574, -13.9539, -14.3494,\n",
      "         -14.7436, -15.1365, -15.5281, -15.9181, -16.3066, -16.6936,\n",
      "         -17.0789, -17.4625, -17.8385, -18.1743, -18.4132, -18.6025,\n",
      "         -18.7466, -18.8515, -18.9240, -18.9541, -18.9432, -18.9004,\n",
      "         -18.8952, -18.8970, -18.9007, -18.9049]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X_position_test, Y_position_test, X_displacement_test, Y_displacement_test = get_random_batches(20, 40, 8)\n",
    "\n",
    "print('X positions:')\n",
    "print(X_position_test.size())\n",
    "print(X_position_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('Y positions:')\n",
    "print(Y_position_test.size())\n",
    "print(Y_position_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('X displacements:')\n",
    "print(X_displacement_test.size())\n",
    "print(X_displacement_test[0])\n",
    "\n",
    "print_line_sep()\n",
    "\n",
    "print('Y displacements:')\n",
    "print(Y_displacement_test.size())\n",
    "print(Y_displacement_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, seq_len, num_layers,\n",
    "                 conv_output_dim, conv_kernel_size,\n",
    "                 gru_hidden_dim):\n",
    "        super(SampleEncoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_output_dim = conv_output_dim\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        # C = X or Y\n",
    "        # C_i, (input_dim, seq_len) -> tC_i, (conv_output_dim, seq_len)\n",
    "        self.conv = nn.Conv1d(input_dim, conv_output_dim, conv_kernel_size)\n",
    "\n",
    "        # tC_i, (conv_output_dim, seq_len) -> H_C_i, (gru_hidden_dim)\n",
    "        self.gru = nn.GRU(conv_output_dim, gru_hidden_dim, num_layers)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initial hidden vector is hidden_dim-dimensional and padded with 0\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.gru_hidden_dim)).cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        conv_output = self.conv(x)\n",
    "        conv_output = F.relu(conv_output)\n",
    "        \n",
    "        # conv_output has dimensions (batch_size, dim, seq_len)\n",
    "        # GRU accepts input tensor with dimensions (seq_len, batch_size, dim)\n",
    "        # TODO: Pad\n",
    "        conv_output = conv_output.permute(2, 0, 1)\n",
    "#         conv_output = conv_output.view(self.seq_len - self.conv_kernel_size + 1,\n",
    "#                                        batch_size,\n",
    "#                                        self.conv_output_dim)\n",
    "        \n",
    "        output, hidden = self.gru(conv_output, hidden)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleCVAE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, mu_dim, sigma_dim):\n",
    "        super(SampleCVAE, self).__init__()\n",
    "        \n",
    "        self.sigma_dim = sigma_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc_mu = nn.Linear(output_dim, mu_dim)\n",
    "        self.fc_sigma = nn.Linear(output_dim, sigma_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        mu = self.fc_mu(output)\n",
    "        sigma = torch.div(torch.exp(self.fc_sigma(output)), 2)\n",
    "        # Reparam trick\n",
    "        epsilon = Variable(torch.normal(torch.zeros(batch_size, self.sigma_dim),\n",
    "                           torch.ones(batch_size, self.sigma_dim))).cuda()\n",
    "\n",
    "        return mu + sigma*epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleFCS(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SampleFCS, self).__init__()\n",
    "        \n",
    "        self.fcs = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fcs(x)\n",
    "        output = F.softmax(output, dim=2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, num_layers, gru_hidden_dim, gru_output_dim, output_dim):\n",
    "        super(SampleDecoder, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, gru_output_dim, num_layers)\n",
    "        self.linear = nn.Linear(gru_output_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.gru_hidden_dim)).cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1\n",
    "batch_size = 512\n",
    "X_seq_len = 20\n",
    "Y_seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "# input_dim = 2        | input is a sequence of (x,y) coordinates (i.e. 2-dimensional)\n",
    "# seq_len = 20         | time sequence length of 20\n",
    "# conv_output_dim = 16 | 1D convolution with 16 output channels\n",
    "# conv_kernel_size = 3 | 1D convolution with kernel of width 3\n",
    "# gru_hidden_dim = 48  | 48-dimensional hidden vector\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 2, 20)\n",
    "encoder1 = SampleEncoder(input_dim=2, seq_len=X_seq_len, num_layers=1,\n",
    "                         conv_output_dim=16, conv_kernel_size=3,\n",
    "                         gru_hidden_dim=48).cuda()\n",
    "\n",
    "# input_dim = 2        | input is a sequence of (x,y) coordinates (i.e. 2-dimensional)\n",
    "# seq_len = 40         | time sequence length of 40\n",
    "# conv_output_dim = 16 | 1D convolution with 16 output channels\n",
    "# conv_kernel_size = 1 | 1D convolution with kernel of width 1\n",
    "# gru_hidden_dim = 48  | 48-dimensional hidden vector\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 2, 40)\n",
    "encoder2 = SampleEncoder(input_dim=2, seq_len=Y_seq_len, num_layers=1,\n",
    "                         conv_output_dim=16, conv_kernel_size=1,\n",
    "                         gru_hidden_dim=48).cuda()\n",
    "\n",
    "# input_dim = 96       | Concatenate encoder1's and encoder2's outputs (48-dim each) into 1 output (96-dim)\n",
    "# output_dim = 48      | Transform concatenated 96-dim vector into 48-dim vector\n",
    "# mu_dim = 48          | mu is 48-dim\n",
    "# sigma_dim = 48       | sigma is 48-dim\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 48)\n",
    "cvae = SampleCVAE(input_dim=96, output_dim=48, mu_dim=48, sigma_dim=48).cuda()\n",
    "\n",
    "# input_dim = 48       |\n",
    "# output_dim = 48      |\n",
    "#\n",
    "# Input: Tensor of size (batch_size, 48)\n",
    "fcs = SampleFCS(input_dim=48, output_dim=48).cuda()\n",
    "\n",
    "# input_dim = 48       |\n",
    "# seq_len = 40         | time sequence length of 40\n",
    "# gru_hidden_dim = 48  |\n",
    "# gru_output_dim = 48  |\n",
    "# output_dim = 2       |\n",
    "#\n",
    "# Input: Tensor of size (40, batch_size, 48)\n",
    "decoder = SampleDecoder(input_dim=48, seq_len=40, num_layers=1,\n",
    "                        gru_hidden_dim=48, gru_output_dim=48,\n",
    "                        output_dim=2).cuda()\n",
    "\n",
    "# Loss\n",
    "kld = nn.KLDivLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': encoder1.parameters()},\n",
    "    {'params': encoder2.parameters()},\n",
    "    {'params': cvae.parameters()},\n",
    "    {'params': fcs.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "], lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) Total Loss: 27.872, KLD Loss: 1.591, MSE Loss: 26.281\n",
      "\n",
      "Generated Y:\n",
      "tensor([[ 229.2972,    1.8844],\n",
      "        [ 338.8314, -100.5499],\n",
      "        [  94.0559,  -62.9926],\n",
      "        ...,\n",
      "        [  33.3856,   -2.3852],\n",
      "        [ 173.5784,    1.8889],\n",
      "        [ 334.7014, -112.0469]], device='cuda:0')\n",
      "--------------------------------------\n",
      "True Y:\n",
      "tensor([[ 228.3590,    1.8557],\n",
      "        [ 338.8500, -100.5800],\n",
      "        [  94.4102,  -62.7520],\n",
      "        ...,\n",
      "        [  33.7844,   -2.4157],\n",
      "        [ 173.5970,    1.8588],\n",
      "        [ 334.7200, -112.0770]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    X_position, Y_position, X_displacement, Y_displacement = get_random_batches(X_seq_len, Y_seq_len, batch_size)\n",
    "    \n",
    "#     writer.add_scalars('data/X', {\n",
    "#         'x': X.permute(0, 2, 1)[0][0][0],\n",
    "#         'y': Y.permute(0, 2, 1)[0][0][0]\n",
    "#     }, epoch)\n",
    "    \n",
    "    running_kld_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder 1\n",
    "    e1_hidden = encoder1.init_hidden(batch_size)\n",
    "    e1_output, e1_last_hidden = encoder1(X_displacement, e1_hidden)\n",
    "    H_X = e1_last_hidden\n",
    "\n",
    "    # Encoder 2\n",
    "    e2_hidden = encoder2.init_hidden(batch_size)\n",
    "    e2_output, e2_last_hidden = encoder2(Y_displacement, e2_hidden)\n",
    "    H_Y = e2_last_hidden\n",
    "\n",
    "    # CVAE\n",
    "    H_XY = torch.cat([H_X, H_Y], 2)\n",
    "    z = cvae(H_XY)\n",
    "\n",
    "    # FCS\n",
    "    beta_z = fcs(z)\n",
    "    \n",
    "    # Decoder\n",
    "    xz = H_X*beta_z\n",
    "    hxz = xz\n",
    "    for i in range(39):\n",
    "        hxz = torch.cat((hxz, Variable(torch.zeros(1, batch_size, 48)).cuda()), 0)\n",
    "    decoder_hidden = decoder.init_hidden(batch_size)\n",
    "    output, last_hidden = decoder(hxz, decoder_hidden)\n",
    "\n",
    "    # Reconstruction\n",
    "    X0 = X_position.permute(2, 0, 1)[-1]\n",
    "    delta_X0 = output[0]    \n",
    "    Y0_hat = X0 + delta_X0\n",
    "    Y_hat = Y0_hat.unsqueeze(0)\n",
    "    \n",
    "    for i in range(1, Y_seq_len):\n",
    "        Yi = Y_hat[i - 1]\n",
    "        delta_Xi = output[i]\n",
    "        Yi_hat = Yi + delta_Xi\n",
    "        Yi_hat = Yi_hat.unsqueeze(0)\n",
    "        Y_hat = torch.cat((Y_hat, Yi_hat), 0)\n",
    "        \n",
    "    # Minimise loss\n",
    "    # KLD Loss requires random z in N(0,1)\n",
    "    test_z = Variable(torch.normal(torch.zeros(batch_size, 48),\n",
    "                                   torch.ones(batch_size, 48))).cuda()\n",
    "    test_z = test_z.unsqueeze(0)\n",
    "    kld_loss = kld(torch.log(beta_z), test_z)\n",
    "    \n",
    "    # MSE Loss requires to add displacement at all steps before\n",
    "    Y_true = Y_position.permute(2, 0, 1)\n",
    "    mse_loss = mse(Y_hat, Y_true)\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = kld_loss + mse_loss\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    running_kld_loss += kld_loss.item()\n",
    "    running_mse_loss += mse_loss.item()\n",
    "    running_loss += loss.item()\n",
    "    print('(Epoch %d) Total Loss: %.3f, KLD Loss: %.3f, MSE Loss: %.3f' \n",
    "          % (epoch + 1, running_loss, running_kld_loss, running_mse_loss))\n",
    "    running_kld_loss = 0.0\n",
    "    running_mse_loss = 0.0\n",
    "    \n",
    "    if epoch + 1 == num_epochs:\n",
    "        print()\n",
    "        \n",
    "        print('Generated Y:')\n",
    "        print(Y_hat[0])\n",
    "        \n",
    "        print_line_sep()\n",
    "        \n",
    "        print('True Y:')\n",
    "        print(Y_true[0])\n",
    "    \n",
    "# writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking & Refinement Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
